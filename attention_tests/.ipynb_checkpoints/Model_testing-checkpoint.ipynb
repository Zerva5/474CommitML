{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f59dfa-ee1e-4237-a328-856946c436e9",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29798ae-db6e-45a4-9db7-dbcd70969279",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "496be0a9-e624-451e-935d-9f883bcc93a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import ast\n",
    "import git\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "576a8a79-7b4a-4ff5-a7bb-389562a808bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['KMP_BLOCKTIME'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "6d353780-89ac-4d93-9006-243760b42127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Attention, Concatenate, Layer, Embedding, Dot, Softmax, TimeDistributed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.callbacks import LambdaCallback\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MultiLabelBinarizer\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.layers import LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.layers import Add, TimeDistributed, Masking\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Dense, Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Masking, Dense, Dropout, LayerNormalization\n",
    "from tensorflow.keras.layers import MultiHeadAttention, TimeDistributed, GlobalMaxPooling1D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6d2832-bb09-4f59-aab3-de43419c99ac",
   "metadata": {},
   "source": [
    "### Setup of metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "0a1d8287-24b5-4ae2-be3d-f371282d3df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_NODE_TYPES = []\n",
    "ALL_NODE_INDEXES = []\n",
    "i = 0\n",
    "for name in dir(ast):\n",
    "    if not name.startswith('_'):\n",
    "        attr = getattr(ast, name)\n",
    "        if isinstance(attr, type) and issubclass(attr, ast.AST):\n",
    "            ALL_NODE_TYPES.append(name)\n",
    "            ALL_NODE_INDEXES.append(i)\n",
    "            i += 1\n",
    "\n",
    "MAX_NODE_LOOKUP_NUM = len(ALL_NODE_TYPES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258055fc-8943-4e62-a78b-e8cefa4f7903",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "92025895-960e-4e27-a74a-8a807e967784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_commit_shas(directory, n):\n",
    "    # Open the Git repository\n",
    "    repo = git.Repo(directory)\n",
    "\n",
    "    # Get the SHA of the latest commit on the main branch\n",
    "    latest_commit_sha = repo.head.commit.hexsha\n",
    "\n",
    "    # Get the SHA of the first commit on the main branch\n",
    "    first_commit_sha = repo.git.rev_list('--max-parents=0', 'main').splitlines()[0]\n",
    "\n",
    "    # Create a list of all the commit SHAs on the main branch\n",
    "    all_commit_shas = [commit.hexsha for commit in repo.iter_commits('main')]\n",
    "\n",
    "    # Exclude the most recent and first ever commits from the list of possible random commit SHAs\n",
    "    possible_commit_shas = [sha for sha in all_commit_shas if sha != latest_commit_sha and sha != first_commit_sha]\n",
    "\n",
    "    # Create an array to store the commit shas\n",
    "    commit_shas = []\n",
    "\n",
    "    # Loop n times to generate n random commit shas\n",
    "    for i in range(n):\n",
    "        # Generate a random index within the range of possible commit SHAs\n",
    "        random_index = random.randint(0, len(possible_commit_shas) - 1)\n",
    "\n",
    "        # Get the commit SHA at the random index\n",
    "        commit_sha = possible_commit_shas[random_index]\n",
    "\n",
    "        # Append the commit SHA to the array\n",
    "        commit_shas.append(commit_sha)\n",
    "\n",
    "        # Remove the chosen commit SHA from the list of possible commit SHAs to avoid duplicates\n",
    "        possible_commit_shas.remove(commit_sha)\n",
    "\n",
    "    # Return the array of commit shas\n",
    "    return commit_shas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "03b1efb4-2ffa-4ba3-8e03-37d03698ca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Commit:\n",
    "    def __init__(self, sha, repo_path):\n",
    "        self.sha = sha\n",
    "        self.repo_path = repo_path\n",
    "        self.parent = None\n",
    "        self.author = None\n",
    "        self.datetime = None\n",
    "        self._populate_commit_info()\n",
    "\n",
    "    def _populate_commit_info(self):\n",
    "        repo = git.Repo(self.repo_path)\n",
    "        commit = repo.commit(self.sha)\n",
    "        self.parent = commit.parents[0].hexsha if commit.parents else None\n",
    "        self.author = commit.author.name if commit.author else None\n",
    "        self.datetime = datetime.fromtimestamp(commit.committed_date)\n",
    "\n",
    "    def get_files_changed(self):\n",
    "        try:\n",
    "            repo = git.Repo(self.repo_path)\n",
    "            commit = repo.commit(self.sha)\n",
    "            return [diff.a_path for diff in commit.diff(commit.parents[0]) if diff.a_path.endswith('.py')]\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    def get_source_at_commit(self, file_name):\n",
    "        try:\n",
    "            repo = git.Repo(self.repo_path)\n",
    "            commit = repo.commit(self.sha)\n",
    "            return commit.tree[file_name].data_stream.read().decode('utf-8')\n",
    "        except:\n",
    "            return ''\n",
    "\n",
    "    def get_source_at_parent(self, file_name):\n",
    "        try:\n",
    "            repo = git.Repo(self.repo_path)\n",
    "            commit = repo.commit(self.sha)\n",
    "            return commit.parents[0].tree[file_name].data_stream.read().decode('utf-8')\n",
    "        except:\n",
    "            return ''\n",
    "\n",
    "    def source_to_ast(self, source):\n",
    "        try:\n",
    "            return ast.parse(source)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def get_paths(self, tree):\n",
    "        try:\n",
    "            paths = set()\n",
    "\n",
    "            # Recursive function to explore the tree\n",
    "            def explore(node, path):\n",
    "                # Add current node to path\n",
    "                path.append(type(node).__name__)\n",
    "\n",
    "                # If the node has no children, it's a leaf node and the path is complete\n",
    "                if not list(ast.iter_child_nodes(node)):\n",
    "                    paths.add(tuple(path))\n",
    "                else:\n",
    "                    # Explore each child node recursively\n",
    "                    for child in ast.iter_child_nodes(node):\n",
    "                        explore(child, path)\n",
    "\n",
    "                # Remove current node from path before returning\n",
    "                path.pop()\n",
    "\n",
    "            # Start exploring from the root node\n",
    "            root = ast.parse(\"\")\n",
    "            explore(tree, [])\n",
    "\n",
    "            return paths\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    def ast_to_bag_of_contexts(self, ast_trees):\n",
    "        paths = set()\n",
    "        for tree in ast_trees:\n",
    "            paths |= self.get_paths(tree)\n",
    "        return paths\n",
    "\n",
    "    def map_bag_of_contexts_to_id(self, bag_of_contexts, all_node_types):\n",
    "        mapped_paths = []\n",
    "        for path in bag_of_contexts:\n",
    "            mapped_path = []\n",
    "            for node in path:\n",
    "                index = all_node_types.index(node)\n",
    "                mapped_path.append(index + 1)\n",
    "            mapped_paths.append(mapped_path)\n",
    "        return mapped_paths\n",
    "\n",
    "    def one_hot_encode(self, bag_of_contexts, max_node_lookup_num):\n",
    "        one_hot_paths = []\n",
    "\n",
    "        # Iterate over each row in the array\n",
    "        for row in bag_of_contexts:\n",
    "            # Create an empty list to hold the one-hot encodings for this row\n",
    "            row_one_hot = []\n",
    "\n",
    "            # Iterate over each element in the row\n",
    "            for num in row:\n",
    "                # Create an empty list to hold the one-hot encoding for this number\n",
    "                num_one_hot = [0] * (max_node_lookup_num + 1)\n",
    "\n",
    "                # Set the corresponding element to 1\n",
    "                num_one_hot[int(num)] = 1\n",
    "\n",
    "                # Add the one-hot encoding for this number to the row's list\n",
    "                row_one_hot.append(num_one_hot)\n",
    "\n",
    "            # Add the row's list of one-hot encodings to the main list\n",
    "            one_hot_paths.append(row_one_hot)\n",
    "\n",
    "        return one_hot_paths\n",
    "\n",
    "    def pad_each_context(self, bag_of_contexts, set_path_length, max_node_lookup_num, one_hot=True):\n",
    "        padded_one_hot_paths = []\n",
    "        for path in bag_of_contexts:\n",
    "            if one_hot:\n",
    "                padded_path = [[0] * (max_node_lookup_num + 1)] * max(set_path_length - len(path), 0) + path[-set_path_length:]\n",
    "            else:\n",
    "                padded_path = [0] * max(set_path_length - len(path), 0) + path[-set_path_length:]\n",
    "            padded_one_hot_paths.append(padded_path)\n",
    "        return padded_one_hot_paths\n",
    "\n",
    "    def to_raw_consumable_data(self, onehot=False, all_node_types=ALL_NODE_TYPES, max_node_lookup_num=MAX_NODE_LOOKUP_NUM, set_path_length=32):\n",
    "        files_changed = self.get_files_changed()\n",
    "        sources_at_commit = [self.get_source_at_commit(filename) for filename in files_changed]\n",
    "        sources_at_parent = [self.get_source_at_parent(filename) for filename in files_changed]        \n",
    "\n",
    "        asts_commit = [self.source_to_ast(source) for source in sources_at_commit]\n",
    "        asts_parent = [self.source_to_ast(source) for source in sources_at_parent]\n",
    "        \n",
    "        contexts_commit = self.ast_to_bag_of_contexts(asts_commit)\n",
    "        contexts_parent = self.ast_to_bag_of_contexts(asts_parent)\n",
    "\n",
    "        contexts = list(contexts_commit.symmetric_difference(contexts_parent))\n",
    "\n",
    "        \n",
    "        contexts = self.map_bag_of_contexts_to_id(contexts, all_node_types)\n",
    "\n",
    "        if(onehot):\n",
    "            contexts = self.one_hot_encode(contexts, max_node_lookup_num)\n",
    "\n",
    "        contexts = self.pad_each_context(contexts, set_path_length, max_node_lookup_num, one_hot=onehot)\n",
    "\n",
    "        return contexts\n",
    "    \n",
    "    def to_padded_consumable_data(self, padded_length=256, onehot=False, all_node_types=ALL_NODE_TYPES, max_node_lookup_num=MAX_NODE_LOOKUP_NUM, set_path_length=32):\n",
    "        consumable = self.to_raw_consumable_data(onehot=onehot, all_node_types=all_node_types, max_node_lookup_num=max_node_lookup_num, set_path_length=set_path_length)\n",
    "\n",
    "        if(len(consumable) == padded_length):\n",
    "            return consumable\n",
    "        \n",
    "        if(len(consumable) > padded_length):\n",
    "            return random.sample(consumable, padded_length)\n",
    "        \n",
    "        if onehot:\n",
    "            blank_path = [[0] * (max_node_lookup_num + 1)] * ([0] * set_path_length)\n",
    "        else:\n",
    "            blank_path = ([0] * set_path_length)\n",
    "            \n",
    "        return ([blank_path] * (padded_length - len(consumable)) + consumable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f0a807-b98d-4633-9bb4-105bb11661a8",
   "metadata": {},
   "source": [
    "### Data Configuration and Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "b49c74f8-1b04-4bdd-be44-4cb6d23b27d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Commit' object has no attribute 'to_raw_consumable_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[258], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m DATA_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m      5\u001b[0m COMMITS \u001b[38;5;241m=\u001b[39m [Commit(sha, REPO_PATH) \u001b[38;5;28;01mfor\u001b[39;00m sha \u001b[38;5;129;01min\u001b[39;00m get_random_commit_shas(REPO_PATH, DATA_SIZE)]\n\u001b[0;32m----> 6\u001b[0m RAW \u001b[38;5;241m=\u001b[39m [c\u001b[38;5;241m.\u001b[39mto_raw_consumable_data() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m COMMITS]\n\u001b[1;32m      7\u001b[0m PADDED \u001b[38;5;241m=\u001b[39m [c\u001b[38;5;241m.\u001b[39mto_padded_consumable_data() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m COMMITS]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m([\u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m RAW])\n",
      "Cell \u001b[0;32mIn[258], line 6\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m DATA_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[1;32m      5\u001b[0m COMMITS \u001b[38;5;241m=\u001b[39m [Commit(sha, REPO_PATH) \u001b[38;5;28;01mfor\u001b[39;00m sha \u001b[38;5;129;01min\u001b[39;00m get_random_commit_shas(REPO_PATH, DATA_SIZE)]\n\u001b[0;32m----> 6\u001b[0m RAW \u001b[38;5;241m=\u001b[39m [\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_raw_consumable_data\u001b[49m() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m COMMITS]\n\u001b[1;32m      7\u001b[0m PADDED \u001b[38;5;241m=\u001b[39m [c\u001b[38;5;241m.\u001b[39mto_padded_consumable_data() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m COMMITS]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m([\u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m RAW])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Commit' object has no attribute 'to_raw_consumable_data'"
     ]
    }
   ],
   "source": [
    "#Set this to any git repo on your local\n",
    "REPO_PATH = \"/home/brennan/bot-radio-tempName\"\n",
    "DATA_SIZE = 32\n",
    "\n",
    "COMMITS = [Commit(sha, REPO_PATH) for sha in get_random_commit_shas(REPO_PATH, DATA_SIZE)]\n",
    "RAW = [c.to_raw_consumable_data() for c in COMMITS]\n",
    "PADDED = [c.to_padded_consumable_data() for c in COMMITS]\n",
    "\n",
    "print([len(x) for x in RAW])\n",
    "print([len(x) for x in PADDED])\n",
    "\n",
    "# X_train, y_train, X, P, d, Y, _, _ = create_dataset(REPO_PATH, DATA_SIZE, onehot=False)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "16f9f039-66db-432c-9870-c819d8434c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "610c99bf-cac5-4bfe-8309-a0c3d4289aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = max([len(seq) for seq in X_train + X_test])\n",
    "X_train = pad_sequences(X_train, maxlen=max_seq_length, padding='post')\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = pad_sequences(X_test, maxlen=max_seq_length, padding='post')\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "1df53e0c-476c-4bf5-a308-b3140ca5adba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "3/3 [==============================] - 15s 4s/step - loss: 0.3288\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 12s 4s/step - loss: 0.3288\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 13s 4s/step - loss: 0.3288\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 13s 4s/step - loss: 0.3288\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 12s 3s/step - loss: 0.3288\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 13s 4s/step - loss: 0.3288\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 13s 4s/step - loss: 0.3288\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 15s 4s/step - loss: 0.3288\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 16s 5s/step - loss: 0.3288\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 15s 5s/step - loss: 0.3288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f29636e2cd0>"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model\n",
    "input_shape = (max_seq_length, len(X_train[0][0]))\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# Apply masking to ignore padded values\n",
    "masked_inputs = Masking()(inputs)\n",
    "\n",
    "# Self-attention layer\n",
    "self_attention = MultiHeadAttention(num_heads=8, key_dim=64)(masked_inputs, masked_inputs)\n",
    "\n",
    "# Layer normalization\n",
    "normalized = LayerNormalization()(self_attention)\n",
    "\n",
    "# Feedforward layer\n",
    "feedforward = TimeDistributed(Dense(units=256, activation='relu'))(normalized)\n",
    "\n",
    "# Global max pooling layer to produce a fixed-length output vector representation\n",
    "pooled = GlobalMaxPooling1D()(feedforward)\n",
    "\n",
    "# Dropout layer to prevent overfitting\n",
    "dropout = Dropout(rate=0.2)(pooled)\n",
    "\n",
    "# Output layer\n",
    "outputs = Dense(units=output_size, activation='softmax')(dropout)\n",
    "\n",
    "# Define model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "7dfa4dc2-5e58-4fc1-878f-0e510999e6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.3497\n",
      "Test loss: 0.34965842962265015\n"
     ]
    }
   ],
   "source": [
    "loss = model.evaluate(X_test, y_test)\n",
    "# Print test loss\n",
    "print(\"Test loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc716d5-2d74-4383-835b-391d01823dac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07161ac-b7cf-4de9-9899-eadb6ce248d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eac426-ee3c-49b7-be6b-41acf3fcb7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6e9c41-1e1b-4513-8db4-4d1e2f0da352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
