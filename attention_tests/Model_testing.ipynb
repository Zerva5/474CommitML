{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4f59dfa-ee1e-4237-a328-856946c436e9",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29798ae-db6e-45a4-9db7-dbcd70969279",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "496be0a9-e624-451e-935d-9f883bcc93a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import ast\n",
    "import git\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "576a8a79-7b4a-4ff5-a7bb-389562a808bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['KMP_BLOCKTIME'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d353780-89ac-4d93-9006-243760b42127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Attention, Concatenate, Layer, Embedding, Dot, Softmax, TimeDistributed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.callbacks import LambdaCallback\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6d2832-bb09-4f59-aab3-de43419c99ac",
   "metadata": {},
   "source": [
    "### Setup of metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a1d8287-24b5-4ae2-be3d-f371282d3df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_NODE_TYPES = []\n",
    "ALL_NODE_INDEXES = []\n",
    "i = 0\n",
    "for name in dir(ast):\n",
    "    if not name.startswith('_'):\n",
    "        attr = getattr(ast, name)\n",
    "        if isinstance(attr, type) and issubclass(attr, ast.AST):\n",
    "            ALL_NODE_TYPES.append(name)\n",
    "            ALL_NODE_INDEXES.append(i)\n",
    "            i += 1\n",
    "\n",
    "MAX_NODE_LOOKUP_NUM = len(ALL_NODE_TYPES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258055fc-8943-4e62-a78b-e8cefa4f7903",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19fe1c98-fa27-49c1-9b55-aff9c6050d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_commit(commit_sha, repo_path, onehot=True, padded=True):\n",
    "    def get_file_contents(commit, file_path):\n",
    "        return commit.tree[file_path].data_stream.read().decode('utf-8')\n",
    "\n",
    "    def get_ast(contents):\n",
    "        return ast.parse(contents)\n",
    "\n",
    "    def get_paths(tree):\n",
    "        paths = set()\n",
    "\n",
    "        # Recursive function to explore the tree\n",
    "        def explore(node, path):\n",
    "            # Add current node to path\n",
    "            path.append(type(node).__name__)\n",
    "\n",
    "            # If the node has no children, it's a leaf node and the path is complete\n",
    "            if not list(ast.iter_child_nodes(node)):\n",
    "                paths.add(tuple(path))\n",
    "            else:\n",
    "                # Explore each child node recursively\n",
    "                for child in ast.iter_child_nodes(node):\n",
    "                    explore(child, path)\n",
    "\n",
    "            # Remove current node from path before returning\n",
    "            path.pop()\n",
    "\n",
    "        # Start exploring from the root node\n",
    "        root = ast.parse(\"\")\n",
    "        explore(tree, [])\n",
    "\n",
    "        return paths\n",
    "\n",
    "    ########################################## Processing and setup ##########################################\n",
    "\n",
    "    repo = git.Repo(repo_path)\n",
    "    commit = repo.commit(commit_sha)\n",
    "    changed_py_files = [diff.a_path for diff in commit.diff(commit.parents[0]) if diff.a_path.endswith('.py')]\n",
    "\n",
    "    ########################################## Compute Abstract Syntax Trees ##########################################\n",
    "\n",
    "    pre_commit_trees = []\n",
    "    post_commit_trees = []\n",
    "\n",
    "    for file_path in changed_py_files:\n",
    "        # print(commit.parents[0], file_path)\n",
    "        try:\n",
    "            pre_commit_contents = get_file_contents(commit.parents[0], file_path)\n",
    "            pre_commit_tree = get_ast(pre_commit_contents)\n",
    "            pre_commit_trees.append(pre_commit_tree)\n",
    "            post_commit_contents = get_file_contents(commit, file_path)\n",
    "            post_commit_tree = get_ast(post_commit_contents)\n",
    "            post_commit_trees.append(post_commit_tree)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    ########################################## Compute Bag of Contexts (paths) ##########################################\n",
    "\n",
    "\n",
    "    pre_commit_paths = set()\n",
    "    for tree in pre_commit_trees:\n",
    "        pre_commit_paths |= get_paths(tree)\n",
    "\n",
    "    post_commit_paths = set()\n",
    "    for tree in post_commit_trees:\n",
    "        post_commit_paths |= get_paths(tree)\n",
    "\n",
    "    unique_paths = pre_commit_paths.symmetric_difference(post_commit_paths)\n",
    "\n",
    "    ########################################## Map Symbols to a number/index/id ##########################################\n",
    "\n",
    "    mapped_paths = []\n",
    "    for path in unique_paths:\n",
    "        mapped_path = []\n",
    "        for node in path:\n",
    "            index = ALL_NODE_TYPES.index(node)\n",
    "            mapped_path.append(index + 1)\n",
    "        mapped_paths.append(mapped_path)\n",
    "\n",
    "    ########################################## Convert to One-Hot Encoding ##########################################\n",
    "\n",
    "    if onehot:\n",
    "        one_hot_paths = []\n",
    "\n",
    "        # Iterate over each row in the array\n",
    "        for row in mapped_paths:\n",
    "            # Create an empty list to hold the one-hot encodings for this row\n",
    "            row_one_hot = []\n",
    "\n",
    "            # Iterate over each element in the row\n",
    "            for num in row:\n",
    "                # Create an empty list to hold the one-hot encoding for this number\n",
    "                num_one_hot = [0] * (MAX_NODE_LOOKUP_NUM+1)\n",
    "\n",
    "                # Set the corresponding element to 1\n",
    "                num_one_hot[int(num)] = 1\n",
    "\n",
    "                # Add the one-hot encoding for this number to the row's list\n",
    "                row_one_hot.append(num_one_hot)\n",
    "\n",
    "            # Add the row's list of one-hot encodings to the main list\n",
    "            one_hot_paths.append(row_one_hot)\n",
    "            \n",
    "            mapped_paths = one_hot_paths\n",
    "\n",
    "\n",
    "    ########################################## Pad to a fixed length ##########################################\n",
    "\n",
    "    if padded:\n",
    "    \n",
    "        padded_one_hot_paths = []\n",
    "\n",
    "        SET_PATH_LENGTH = 32\n",
    "\n",
    "        for path in mapped_paths:\n",
    "            if onehot:\n",
    "                padded_path = [[0] * (MAX_NODE_LOOKUP_NUM+1)] * max(SET_PATH_LENGTH - len(path), 0) + path[-SET_PATH_LENGTH:]\n",
    "            else:\n",
    "                padded_path = [0] * max(SET_PATH_LENGTH - len(path), 0) + path[-SET_PATH_LENGTH:]\n",
    "            padded_one_hot_paths.append(padded_path)\n",
    "    \n",
    "        return padded_one_hot_paths\n",
    "    else:\n",
    "        return mapped_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "92025895-960e-4e27-a74a-8a807e967784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_commit_shas(directory, n):\n",
    "    # Open the Git repository\n",
    "    repo = git.Repo(directory)\n",
    "\n",
    "    # Get the SHA of the latest commit on the main branch\n",
    "    latest_commit_sha = repo.head.commit.hexsha\n",
    "\n",
    "    # Get the SHA of the first commit on the main branch\n",
    "    first_commit_sha = repo.git.rev_list('--max-parents=0', 'main').splitlines()[0]\n",
    "\n",
    "    # Create a list of all the commit SHAs on the main branch\n",
    "    all_commit_shas = [commit.hexsha for commit in repo.iter_commits('main')]\n",
    "\n",
    "    # Exclude the most recent and first ever commits from the list of possible random commit SHAs\n",
    "    possible_commit_shas = [sha for sha in all_commit_shas if sha != latest_commit_sha and sha != first_commit_sha]\n",
    "\n",
    "    # Create an array to store the commit shas\n",
    "    commit_shas = []\n",
    "\n",
    "    # Loop n times to generate n random commit shas\n",
    "    for i in range(n):\n",
    "        # Generate a random index within the range of possible commit SHAs\n",
    "        random_index = random.randint(0, len(possible_commit_shas) - 1)\n",
    "\n",
    "        # Get the commit SHA at the random index\n",
    "        commit_sha = possible_commit_shas[random_index]\n",
    "\n",
    "        # Append the commit SHA to the array\n",
    "        commit_shas.append(commit_sha)\n",
    "\n",
    "        # Remove the chosen commit SHA from the list of possible commit SHAs to avoid duplicates\n",
    "        possible_commit_shas.remove(commit_sha)\n",
    "\n",
    "    # Return the array of commit shas\n",
    "    return commit_shas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a6fa198a-ed2f-40ba-b94c-5f6a5284e27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataset(path, size=32, threshold=50, onehot=True):\n",
    "    commits = get_random_commit_shas(path, size)\n",
    "    X_train = []\n",
    "    for commit in tqdm(commits, desc='Processing commits'):\n",
    "        X_train.append(process_commit(commit, path, onehot))\n",
    "    X_train = [data for data in X_train if len(data) > 0]\n",
    "    y_train = [(1 if len(x) > threshold else 0) for x in X_train]\n",
    "\n",
    "    # Determine the input and output dimensions\n",
    "    try:\n",
    "        input_dim = len(X_train[0][0])\n",
    "    except:\n",
    "        input_dim = 1        \n",
    "    try:\n",
    "        output_dim = len(X_train[0][0][0])\n",
    "    except:\n",
    "        output_dim = 1        \n",
    "\n",
    "    X = []\n",
    "\n",
    "    for num in ALL_NODE_INDEXES:\n",
    "        num_one_hot = [0] * (MAX_NODE_LOOKUP_NUM+1)\n",
    "        num_one_hot[int(num)] = 1\n",
    "        if onehot:\n",
    "            X.append(num_one_hot)\n",
    "        else:\n",
    "            X.append(num)\n",
    "\n",
    "    P = [item for sublist in X_train for item in sublist]\n",
    "    P = [arr for i, arr in enumerate(tqdm(P, desc=\"Generating path vocab\")) if arr not in P[:i]]\n",
    "\n",
    "    Y = [0,1]\n",
    "\n",
    "    d=150\n",
    "\n",
    "    return X_train, y_train, X, P, d, Y, input_dim, output_dim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556c8f99-d97d-4a4d-8f8c-557879bf965c",
   "metadata": {},
   "source": [
    "### Data Configuration and Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b49c74f8-1b04-4bdd-be44-4cb6d23b27d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing commits: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:02<00:00, 12.64it/s]\n",
      "Generating path vocab: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 839/839 [00:00<00:00, 855.72it/s]\n"
     ]
    }
   ],
   "source": [
    "#Set this to any git repo on your local\n",
    "REPO_PATH = \"/home/brennan/bot-radio-tempName\"\n",
    "DATA_SIZE = 32\n",
    "\n",
    "X_train, y_train, X, P, d, Y, _, _ = create_dataset(REPO_PATH, DATA_SIZE)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "53e0422e-8680-4722-a164-fc7ea2e8b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_vocab = np.random.randn(len(P), d)\n",
    "value_vocab = np.random.randn(len(X), d)\n",
    "W = np.random.randn(d, 3*d)\n",
    "attention_vector = np.random.randn(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0d509c8a-2ff1-4a46-a14d-a790716cec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_attention_code_vector(path_contexts):\n",
    "    \n",
    "    # Map each path-context to its corresponding embedding\n",
    "    context_vectors = []\n",
    "    for path_context in path_contexts:\n",
    "        pj = path_context\n",
    "        xs = path_context[0]\n",
    "        xt = path_context[-1]\n",
    "\n",
    "        xs_embedding = value_vocab[xs]\n",
    "        pj_embedding = path_vocab[P.index(pj)]\n",
    "        xt_embedding = value_vocab[xs] \n",
    "\n",
    "        context_vector = tf.concat([xs_embedding, pj_embedding, xt_embedding], axis=0)\n",
    "        context_vectors.append(context_vector)\n",
    "\n",
    "    # Combine context vectors using fully connected layer\n",
    "    context_weights = tf.matmul(context_vectors, tf.transpose(W))\n",
    "    combined_context_vectors = tf.nn.tanh(context_weights)\n",
    "\n",
    "    # Compute attention weights\n",
    "    attention_weights = tf.nn.softmax(tf.matmul(combined_context_vectors, tf.expand_dims(attention_vector, axis=1)), axis=0)\n",
    "    \n",
    "    # Aggregate into code vector using attention\n",
    "    code_vector = tf.reduce_sum(tf.multiply(combined_context_vectors, attention_weights), axis=0)\n",
    "    \n",
    "    return code_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610c99bf-cac5-4bfe-8309-a0c3d4289aed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
