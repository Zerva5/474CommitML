{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c44c23b9-60df-4339-9ddd-b7808fb98f35",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GH Access Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c2410bd-ab7f-4cb0-9918-c774f3a22ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_token(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        token = f.read().strip()\n",
    "    return token\n",
    "\n",
    "GH_ACCESS_TOKEN=load_token(\"gh_access_token.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818e479d-9ae0-4728-a87e-8a30b2a7dc92",
   "metadata": {},
   "source": [
    "1. Log in to your GitHub account.\n",
    "2. Click on your profile picture in the top-right corner of the screen and select \"Settings\".\n",
    "3. In the left sidebar, click on \"Developer settings\".\n",
    "4. Click on \"Personal access tokens\".\n",
    "5. Click on \"Generate new token\".\n",
    "6. Give your token a description that will help you remember what it is used for.\n",
    "7. Select the permissions that your token requires. Note that you should only select the permissions that are necessary for your application to function.\n",
    "8. Click on \"Generate token\".\n",
    "9. Copy the generated access token.\n",
    "10. Open a text editor and create a new file called gh_access_token.txt.\n",
    "11. Paste the access token into the file.\n",
    "12. Save and close the file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b4efe-9400-4901-b189-9899fde73415",
   "metadata": {},
   "source": [
    "### Regular Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26140cd4-ed2b-40cd-80b3-efa7bbaae0d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import ast\n",
    "import git\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm, trange\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, wait, ALL_COMPLETED\n",
    "import networkx as nx\n",
    "import tree_sitter\n",
    "from tree_sitter import Language\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from github import Github\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce25b8f6-e77c-46af-94c4-2d8eb95698c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Disable import warnings for tensorflow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['OMP_NUM_THREADS'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10645da7-f5a0-4a21-81c5-564c37fffc49",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Machine Learning Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5af6eac8-dd15-4061-812c-e6d3fb862d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, LSTM, Attention, Concatenate, Layer,\n",
    "    Embedding, Dot, Softmax, TimeDistributed, Multiply,\n",
    "    Lambda, LayerNormalization, MultiHeadAttention,\n",
    "    Add, Masking, GlobalMaxPooling1D, GlobalMaxPooling2D, Reshape, MaxPooling1D, MaxPooling2D,\n",
    "    Dropout, Conv1D, Conv2D, Bidirectional, GRU, ConvLSTM2D, Flatten, Permute\n",
    ")\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import LambdaCallback, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MultiLabelBinarizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import MeanSquaredError, CosineSimilarity\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc241d3-5342-4b4c-8cd0-df6ad1410eab",
   "metadata": {},
   "source": [
    "### Data Pre-Processing Parameters\n",
    "These must be set before running the data preprocessing\n",
    "If any of these parameters are changed, all cells below **must** be re-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b27cdb5-80c9-4e0e-9e08-020b16d46caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of contexts to pad/reduce to per sample\n",
    "#This is the `k` number described in commit2seq\n",
    "BAG_SIZE = 200\n",
    "\n",
    "#Maximum depth of context\n",
    "#This is the depth of the path itself, so with a depth of 32, paths can be a maximum of 32 notes deep.\n",
    "#Smaller paths mean a higher focus on local dependencies, while longer paths represent a more general\n",
    "#representation of long-distance dependencies.\n",
    "CONTEXT_SIZE = 16\n",
    "\n",
    "#Internal fixed length representation size. This is the size of the fixed-length vector that the encoder\n",
    "#will learn to produce. Experimentation will be needed.\n",
    "OUTPUT_SIZE = 2048\n",
    "\n",
    "#Toggle to determine if tokens should be one-hot encoded or not\n",
    "ONE_HOT = False\n",
    "\n",
    "#Toogle to use root -> terminal paths instead of terminal -> terminal paths. \n",
    "#code2vec and commit2vec use terminal -> terminal paths, but generating them is significantly slower.\n",
    "ONLY_ROOT_PATHS = False\n",
    "\n",
    "#Toggle to drop paths above the context size limit. Likely should always be set to true.\n",
    "IGNORE_DEEP_PATHS = True\n",
    "\n",
    "#Generate full tuples of (terminal_token_a, (path...), terminal_token_b) instead of just paths.\n",
    "#commit2seq describes using this strategy as being quite a bit more effective (like 30%), but currently I\n",
    "#haven't figured out how to implement it.\n",
    "USE_FULL_TUPLES = False    #NOT working yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20803f20-9112-4fab-a6d6-b9a3879e7aa2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Token Vocab and Meta Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f25db691-0e1b-4a13-b172-fa0c69004123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load language files\n",
    "#The ast.so file must be re-generated to add support for more languages.\n",
    "#To do this, see: https://github.com/tree-sitter/py-tree-sitter\n",
    "\n",
    "JS_LANGUAGE = Language('ast-bindings/build/ast.so', 'javascript')\n",
    "PY_LANGUAGE = Language('ast-bindings/build/ast.so', 'python')\n",
    "JAVA_LANGUAGE = Language('ast-bindings/build/ast.so', 'java')\n",
    "C_LANGUAGE = Language('ast-bindings/build/ast.so', 'c')\n",
    "CPP_LANGUAGE = Language('ast-bindings/build/ast.so', 'cpp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5eb6c84-89d2-4798-aa86-3c6503bf17ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_json_data(lang):\n",
    "    url = f\"https://raw.githubusercontent.com/tree-sitter/tree-sitter-{lang}/master/src/node-types.json\"\n",
    "    data = json.loads(requests.get(url).text)\n",
    "    types = [node_type[\"type\"] for node_type in data]\n",
    "    for node_type in data:\n",
    "        if \"subtypes\" in node_type:\n",
    "            subtypes = [subtype[\"type\"] for subtype in node_type[\"subtypes\"]]\n",
    "            types.extend(subtypes)\n",
    "    types = list(set(types))\n",
    "    return types\n",
    "\n",
    "PYTHON_NODE_TYPES = download_json_data(\"python\")\n",
    "JAVA_NODE_TYPES = download_json_data(\"java\")\n",
    "JAVASCRIPT_NODE_TYPES = download_json_data(\"javascript\")\n",
    "C_NODE_TYPES = download_json_data(\"c\")\n",
    "CPP_NODE_TYPES = download_json_data(\"cpp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "248a6047-1fef-4c81-96d9-655140cc2a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For now set to just use JAVA node types. To enable support for other languages, just use a set union.\n",
    "ALL_NODE_TYPES = JAVA_NODE_TYPES\n",
    "#ALL_NODE_TYPES = list(set(python_data + java_data + javascript_data + c_data + cpp_data))\n",
    "\n",
    "# FILE_FILTERS = (\".c\", \".cpp\", \".java\", \".js\", \".py\")\n",
    "FILE_FILTERS = (\".java\")\n",
    "\n",
    "MAX_NODE_LOOKUP_NUM = len(ALL_NODE_TYPES)\n",
    "ALL_NODE_INDEXES = range(MAX_NODE_LOOKUP_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4366bc8c-97f9-43bf-898d-93ad374c28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automatically generated INPUT_SHAPE param. Don't modify\n",
    "if(ONE_HOT):\n",
    "    INPUT_SHAPE = (BAG_SIZE, CONTEXT_SIZE * (MAX_NODE_LOOKUP_NUM + 1))\n",
    "else:\n",
    "    INPUT_SHAPE = (BAG_SIZE, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db8769b-5427-4108-a6e9-c7bc94ecef34",
   "metadata": {},
   "source": [
    "### Load Raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c4aded3-0ed4-4dd6-8100-d40942c62f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA = pd.read_csv(\"pairs_apache_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37f1d401-c197-4a91-8228-fd243931f419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fix_hash</th>\n",
       "      <th>bug_hash</th>\n",
       "      <th>Y</th>\n",
       "      <th>bug_repo</th>\n",
       "      <th>fix_repo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0b05d3b939c5ed37a4253e7c3614d824e76ed664</td>\n",
       "      <td>0b92cec1e07a1f2d9aa70f3ecd7d0fb12290d2e2</td>\n",
       "      <td>1</td>\n",
       "      <td>apache/kafka</td>\n",
       "      <td>apache/kafka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f84bf9cbe771f252d5624e30d27755c9e5225179</td>\n",
       "      <td>d46e5634a3bca248e00b4f44e5216cd6607c5a52</td>\n",
       "      <td>1</td>\n",
       "      <td>apache/hbase</td>\n",
       "      <td>apache/hbase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6c8f14b78afc0a6721433af6554b0ab45b8e163d</td>\n",
       "      <td>53989648276fc057d5ec7ab056a7d0a654d110b8</td>\n",
       "      <td>1</td>\n",
       "      <td>apache/hive</td>\n",
       "      <td>apache/hive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9623ae4de3e4c65ab2e90798843769795b8a00c6</td>\n",
       "      <td>505cf9d618c24f532e7b800d8b34a20a40c45feb</td>\n",
       "      <td>1</td>\n",
       "      <td>apache/kafka</td>\n",
       "      <td>apache/kafka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9fe8802cb83e05c0392b11b8dcfe354fecfda786</td>\n",
       "      <td>2e5c28f8c927f2caf6dcffc9070671089c5f771f</td>\n",
       "      <td>1</td>\n",
       "      <td>apache/hive</td>\n",
       "      <td>apache/hive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>16c934812b2395da7fb3968965b03d2f6aa8c8a3</td>\n",
       "      <td>0a08525ad236f78df05c854dead62f300eae271d</td>\n",
       "      <td>0</td>\n",
       "      <td>apache/cassandra</td>\n",
       "      <td>apache/cassandra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>b04bed022aec0f1f478a03383ab5184f048133b6</td>\n",
       "      <td>44f6c4b946511ce4663d41bf40f2960d2faee198</td>\n",
       "      <td>0</td>\n",
       "      <td>apache/kafka</td>\n",
       "      <td>apache/kafka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>b04bed022aec0f1f478a03383ab5184f048133b6</td>\n",
       "      <td>0d4cf64af359d22749f7e865c4efaee773d64962</td>\n",
       "      <td>0</td>\n",
       "      <td>apache/kafka</td>\n",
       "      <td>apache/kafka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>925599c72bc35edb5ffa25c6cc839932573b01aa</td>\n",
       "      <td>a8a73362bd9a3967c46011ded1ed831a586acd2e</td>\n",
       "      <td>0</td>\n",
       "      <td>apache/zookeeper</td>\n",
       "      <td>apache/zookeeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>925599c72bc35edb5ffa25c6cc839932573b01aa</td>\n",
       "      <td>5a6535bf410ed223aac075241045ccd807b17b81</td>\n",
       "      <td>0</td>\n",
       "      <td>apache/zookeeper</td>\n",
       "      <td>apache/zookeeper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>882 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     fix_hash  \\\n",
       "0    0b05d3b939c5ed37a4253e7c3614d824e76ed664   \n",
       "1    f84bf9cbe771f252d5624e30d27755c9e5225179   \n",
       "2    6c8f14b78afc0a6721433af6554b0ab45b8e163d   \n",
       "3    9623ae4de3e4c65ab2e90798843769795b8a00c6   \n",
       "4    9fe8802cb83e05c0392b11b8dcfe354fecfda786   \n",
       "..                                        ...   \n",
       "877  16c934812b2395da7fb3968965b03d2f6aa8c8a3   \n",
       "878  b04bed022aec0f1f478a03383ab5184f048133b6   \n",
       "879  b04bed022aec0f1f478a03383ab5184f048133b6   \n",
       "880  925599c72bc35edb5ffa25c6cc839932573b01aa   \n",
       "881  925599c72bc35edb5ffa25c6cc839932573b01aa   \n",
       "\n",
       "                                     bug_hash  Y          bug_repo  \\\n",
       "0    0b92cec1e07a1f2d9aa70f3ecd7d0fb12290d2e2  1      apache/kafka   \n",
       "1    d46e5634a3bca248e00b4f44e5216cd6607c5a52  1      apache/hbase   \n",
       "2    53989648276fc057d5ec7ab056a7d0a654d110b8  1       apache/hive   \n",
       "3    505cf9d618c24f532e7b800d8b34a20a40c45feb  1      apache/kafka   \n",
       "4    2e5c28f8c927f2caf6dcffc9070671089c5f771f  1       apache/hive   \n",
       "..                                        ... ..               ...   \n",
       "877  0a08525ad236f78df05c854dead62f300eae271d  0  apache/cassandra   \n",
       "878  44f6c4b946511ce4663d41bf40f2960d2faee198  0      apache/kafka   \n",
       "879  0d4cf64af359d22749f7e865c4efaee773d64962  0      apache/kafka   \n",
       "880  a8a73362bd9a3967c46011ded1ed831a586acd2e  0  apache/zookeeper   \n",
       "881  5a6535bf410ed223aac075241045ccd807b17b81  0  apache/zookeeper   \n",
       "\n",
       "             fix_repo  \n",
       "0        apache/kafka  \n",
       "1        apache/hbase  \n",
       "2         apache/hive  \n",
       "3        apache/kafka  \n",
       "4         apache/hive  \n",
       "..                ...  \n",
       "877  apache/cassandra  \n",
       "878      apache/kafka  \n",
       "879      apache/kafka  \n",
       "880  apache/zookeeper  \n",
       "881  apache/zookeeper  \n",
       "\n",
       "[882 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAW_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81410104-2836-48e3-ae38-2e507197900d",
   "metadata": {},
   "source": [
    "### Load Rich Dataset from GH/git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7e58412-d70f-4531-bc94-275243a319ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc617ceac58f405796980a4f3a024e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "repos = set(list(RAW_DATA[\"bug_repo\"]) + list(RAW_DATA[\"fix_repo\"]))\n",
    "#Create a \"clones\" directory in order to clone local repos\n",
    "if not os.path.exists(\"clones\"):\n",
    "    os.makedirs(\"clones\")\n",
    "\n",
    "# Clone each \"repo\" into the \"clones\" folder\n",
    "for repo in tqdm(repos):\n",
    "    repo_folder = repo.replace(\"/\", \"-\")\n",
    "    if not os.path.exists(f\"clones/{repo_folder}\"):\n",
    "        os.system(f\"git clone https://github.com/{repo}.git clones/{repo_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1b64447-5f98-4b9c-9b7d-c87e063c27a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Commit:\n",
    "    \n",
    "    ###################################### SETUP ###################################### \n",
    "    \n",
    "    def __init__(self, sha, repo_path):\n",
    "        self.sha = sha\n",
    "        self.repo_path = repo_path\n",
    "        self.parent = None\n",
    "        self.author = None\n",
    "        self.datetime = None\n",
    "        self.message = None\n",
    "        self.bag_of_contexts = None\n",
    "        self._populate_commit_info()\n",
    "\n",
    "    def _populate_commit_info(self):\n",
    "        repo = git.Repo(self.repo_path)\n",
    "        commit = repo.commit(self.sha)\n",
    "        self.parent = commit.parents[0].hexsha if commit.parents else None\n",
    "        self.author = commit.author.name if commit.author else None\n",
    "        self.datetime = datetime.fromtimestamp(commit.committed_date)\n",
    "        self.message = commit.message if commit.message else None\n",
    "    \n",
    "    def _GH_populate_commit_info(self):\n",
    "        g = Github(GH_ACCESS_TOKEN)\n",
    "        repo = g.get_repo(self.repo_name)\n",
    "        commit = repo.get_commit(sha=self.sha)\n",
    "        self.parent = commit.parents[0].sha if commit.parents else None\n",
    "        self.author = commit.author.name if commit.author else None\n",
    "        self.datetime = commit.commit.author.date\n",
    "        self.message = commit.commit.message if commit.commit.message else None\n",
    "        \n",
    "    def _generate_bags_of_contexts(self):\n",
    "        self.bag_of_contexts = self.to_padded_consumable_data()\n",
    "\n",
    "    ###################################### GIT and GitHub ###################################### \n",
    "        \n",
    "    ################# GIT #################\n",
    "    \n",
    "    def get_files_changed(self):\n",
    "        try:\n",
    "            repo = git.Repo(self.repo_path)\n",
    "            commit = repo.commit(self.sha)\n",
    "            return [diff.a_path for diff in commit.diff(commit.parents[0]) if diff.a_path.endswith(FILE_FILTERS)]\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    def get_source_at_commit(self, file_name):\n",
    "        try:\n",
    "            repo = git.Repo(self.repo_path)\n",
    "            commit = repo.commit(self.sha)\n",
    "            return commit.tree[file_name].data_stream.read().decode('utf-8')\n",
    "        except:\n",
    "            return ''\n",
    "\n",
    "    def get_source_at_parent(self, file_name):\n",
    "        try:\n",
    "            repo = git.Repo(self.repo_path)\n",
    "            commit = repo.commit(self.sha)\n",
    "            return commit.parents[0].tree[file_name].data_stream.read().decode('utf-8')\n",
    "        except:\n",
    "            return ''\n",
    "    \n",
    "    ################# GitHub #################\n",
    "    \n",
    "    def gh_get_files_changed(self):\n",
    "        try:\n",
    "            g = Github(GH_ACCESS_TOKEN)\n",
    "            repo = g.get_repo(self.repo_name)\n",
    "            commit = repo.get_commit(sha=self.sha)\n",
    "            return [f.filename for f in commit.files if f.filename.endswith(FILE_FILTERS)]\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    def gh_get_source_at_commit(self, file_name):\n",
    "        try:\n",
    "            g = Github(GH_ACCESS_TOKEN)\n",
    "            repo = g.get_repo(self.repo_name)\n",
    "            contents = repo.get_contents(file_name, ref=self.sha)\n",
    "            return base64.b64decode(contents.content).decode('utf-8')\n",
    "        except:\n",
    "            return ''\n",
    "\n",
    "    def gh_get_source_at_parent(self, file_name):\n",
    "        try:\n",
    "            g = Github(GH_ACCESS_TOKEN)\n",
    "            repo = g.get_repo(self.repo_name)\n",
    "            commit = repo.get_commit(sha=self.sha)\n",
    "            parent_commit = repo.get_commit(sha=commit.parents[0].sha)\n",
    "            contents = repo.get_contents(file_name, ref=parent_commit.sha)\n",
    "            return base64.b64decode(contents.content).decode('utf-8')\n",
    "        except:\n",
    "            return ''\n",
    "\n",
    "    ###################################### AST Processing ###################################### \n",
    "\n",
    "    def source_to_ast(self, source, file_name):\n",
    "        try:\n",
    "            parser = tree_sitter.Parser()\n",
    "            if file_name.endswith('.c'):\n",
    "                parser.set_language(C_LANGUAGE)\n",
    "            elif file_name.endswith('.cpp'):\n",
    "                parser.set_language(CPP_LANGUAGE)\n",
    "            elif file_name.endswith('.java'):\n",
    "                parser.set_language(JAVA_LANGUAGE)\n",
    "            elif file_name.endswith('.js'):\n",
    "                parser.set_language(JS_LANGUAGE)\n",
    "            elif file_name.endswith('.py'):\n",
    "                parser.set_language(PY_LANGUAGE)\n",
    "            else:\n",
    "                print(\"UNKNOWN LANGUAGE\")\n",
    "                return None\n",
    "            return parser.parse(bytes(source, 'utf8'))\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    ###################################### Bag of Contexts Processing ###################################### \n",
    "        \n",
    "    def get_root_paths(self, tree):\n",
    "        try:\n",
    "            paths = set()\n",
    "\n",
    "            # Recursive function to explore the tree\n",
    "            def explore(node, path, terminalA=None):\n",
    "                # Add current node to path\n",
    "                if terminalA is None:\n",
    "                    terminalA = node\n",
    "                path.append(node.type)\n",
    "\n",
    "                # If the node has no children, it's a leaf node and the path is complete\n",
    "                if not node.child_count:\n",
    "                    if USE_FULL_TUPLES:\n",
    "                        paths.add((terminalA.text, tuple(path), node.text))\n",
    "                    else:\n",
    "                        paths.add(tuple(path))\n",
    "                else:\n",
    "                    # Explore each child node recursively\n",
    "                    for child in node.children:\n",
    "                        explore(child, path, terminalA)\n",
    "\n",
    "                # Remove current node from path before returning\n",
    "                path.pop()\n",
    "\n",
    "            # Start exploring from the root node\n",
    "            root_node = tree.root_node\n",
    "            explore(root_node, [])\n",
    "            \n",
    "            return paths\n",
    "        except:\n",
    "            return set([])\n",
    "\n",
    "    def get_paths(self, ast):\n",
    "        graph = nx.Graph()\n",
    "        node_id = 0\n",
    "        leaf_nodes = []\n",
    "\n",
    "        def add_node(node):\n",
    "            nonlocal node_id\n",
    "            node_name = node.type\n",
    "            node_id += 1\n",
    "            graph.add_node(node_id, name=node_name, node=node, text=node.text)\n",
    "            if not node.children:\n",
    "                leaf_nodes.append(node_id)\n",
    "            return node_id\n",
    "\n",
    "        def add_edge(parent_id, child_id):\n",
    "            graph.add_edge(parent_id, child_id)\n",
    "\n",
    "        def traverse(node, parent_id=None):\n",
    "            current_id = add_node(node)\n",
    "            if parent_id is not None:\n",
    "                add_edge(parent_id, current_id)\n",
    "            for child in node.children:\n",
    "                traverse(child, current_id)\n",
    "\n",
    "        traverse(ast.root_node)\n",
    "\n",
    "        if BAG_SIZE != None:\n",
    "            leaf_nodes_sample = random.sample(leaf_nodes, min(BAG_SIZE, len(leaf_nodes)))\n",
    "        else:\n",
    "            leaf_nodes_sample = leaf_nodes\n",
    "        leaf_node_pairs = [(leaf_nodes_sample[i], leaf_nodes_sample[j]) for i in range(len(leaf_nodes_sample)) for j in range(i+1, len(leaf_nodes_sample))]\n",
    "        \n",
    "        if BAG_SIZE != None:\n",
    "            leaf_node_pairs = random.sample(leaf_node_pairs, min(BAG_SIZE,len(leaf_node_pairs)))\n",
    "        \n",
    "        all_paths = []\n",
    "        for pair in leaf_node_pairs:\n",
    "            path = list(nx.all_simple_paths(graph, source=pair[0], target=pair[1]))[0]\n",
    "            node_types = [graph.nodes[nodeID]['name'] for nodeID in path]\n",
    "            if(USE_FULL_TUPLES):\n",
    "                all_paths.append((graph.nodes[pair[0]]['text'], tuple(node_types), graph.nodes[pair[1]]['text']))\n",
    "            else:\n",
    "                all_paths.append(tuple(node_types))\n",
    "                \n",
    "        return set(all_paths)\n",
    "\n",
    "\n",
    "    def ast_to_bag_of_contexts(self, ast_trees):\n",
    "        paths = set()\n",
    "        for tree in ast_trees:\n",
    "            if(ONLY_ROOT_PATHS):\n",
    "                paths |= self.get_root_paths(tree)\n",
    "            else:\n",
    "                paths |= self.get_paths(tree)\n",
    "        return paths\n",
    "\n",
    "    ###################################### Padding and Encoding ###################################### \n",
    "    \n",
    "    def map_bag_of_contexts_to_id(self, bag_of_contexts):\n",
    "        mapped_paths = []\n",
    "        for path in bag_of_contexts:\n",
    "            mapped_path = []\n",
    "            for node in path:\n",
    "                index = ALL_NODE_TYPES.index(node)\n",
    "                mapped_path.append(index + 1)\n",
    "            mapped_paths.append(mapped_path)\n",
    "        return mapped_paths\n",
    "\n",
    "    def one_hot_encode(self, bag_of_contexts):\n",
    "        one_hot_paths = []\n",
    "\n",
    "        # Iterate over each row in the array\n",
    "        for row in bag_of_contexts:\n",
    "            # Create an empty list to hold the one-hot encodings for this row\n",
    "            row_one_hot = []\n",
    "\n",
    "            # Iterate over each element in the row\n",
    "            for num in row:\n",
    "                # Create an empty list to hold the one-hot encoding for this number\n",
    "                num_one_hot = [0] * (MAX_NODE_LOOKUP_NUM + 1)\n",
    "\n",
    "                # Set the corresponding element to 1\n",
    "                num_one_hot[int(num)] = 1\n",
    "\n",
    "                # Add the one-hot encoding for this number to the row's list\n",
    "                row_one_hot.append(num_one_hot)\n",
    "\n",
    "            # Add the row's list of one-hot encodings to the main list\n",
    "            one_hot_paths.append(row_one_hot)\n",
    "\n",
    "        return one_hot_paths\n",
    "\n",
    "    def pad_each_context(self, bag_of_contexts):\n",
    "        padded_one_hot_paths = []\n",
    "        for path in bag_of_contexts:\n",
    "            if IGNORE_DEEP_PATHS and len(path) > CONTEXT_SIZE:\n",
    "                continue\n",
    "            if ONE_HOT:\n",
    "                padded_path = [[0] * (MAX_NODE_LOOKUP_NUM + 1)] * max(CONTEXT_SIZE - len(path), 0) + path[-CONTEXT_SIZE:]\n",
    "            else:\n",
    "                padded_path = [0] * max(CONTEXT_SIZE - len(path), 0) + path[-CONTEXT_SIZE:]\n",
    "            padded_one_hot_paths.append(padded_path)\n",
    "        return padded_one_hot_paths\n",
    "\n",
    "    ###################################### Utility ###################################### \n",
    "    \n",
    "    def to_raw_consumable_data(self):\n",
    "        files_changed = self.get_files_changed()\n",
    "        sources_at_commit = [self.get_source_at_commit(filename) for filename in files_changed]\n",
    "        sources_at_parent = [self.get_source_at_parent(filename) for filename in files_changed]        \n",
    "\n",
    "        asts_commit = [self.source_to_ast(source, files_changed[i]) for i, source in enumerate(sources_at_commit)]\n",
    "        asts_parent = [self.source_to_ast(source, files_changed[i]) for i, source in enumerate(sources_at_parent)]\n",
    "        \n",
    "        contexts_commit = self.ast_to_bag_of_contexts(asts_commit)\n",
    "        contexts_parent = self.ast_to_bag_of_contexts(asts_parent)\n",
    "\n",
    "        contexts = list(contexts_commit.symmetric_difference(contexts_parent))\n",
    "\n",
    "        \n",
    "        contexts = self.map_bag_of_contexts_to_id(contexts)\n",
    "\n",
    "        if(ONE_HOT):\n",
    "            contexts = self.one_hot_encode(contexts)\n",
    "\n",
    "        contexts = self.pad_each_context(contexts)\n",
    "\n",
    "        return contexts\n",
    "    \n",
    "    def raw_to_padded(self, consumable):\n",
    "        if(len(consumable) == BAG_SIZE):\n",
    "            return consumable\n",
    "        \n",
    "        if(len(consumable) > BAG_SIZE):\n",
    "            return random.sample(consumable, BAG_SIZE)\n",
    "        \n",
    "        if ONE_HOT:\n",
    "            blank_path = [[0] * (MAX_NODE_LOOKUP_NUM + 1)] * CONTEXT_SIZE\n",
    "        else:\n",
    "            blank_path = ([0] * CONTEXT_SIZE)\n",
    "            \n",
    "        return ([blank_path] * (BAG_SIZE - len(consumable)) + consumable)\n",
    "\n",
    "    \n",
    "    def to_padded_consumable_data(self):\n",
    "        consumable = self.to_raw_consumable_data()\n",
    "        padded = self.raw_to_padded(consumable)\n",
    "        if(ONE_HOT):\n",
    "            padded = np.array(padded)\n",
    "            padded = padded.reshape(padded.shape[0], padded.shape[1] * padded.shape[2])\n",
    "            padded = padded.tolist()\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d7d0781-6305-4bd4-873c-0b6979a4db19",
   "metadata": {},
   "outputs": [],
   "source": [
    "TUPLES = [(row[0], row[4]) for row in RAW_DATA.itertuples(index=False)] + [(row[1], row[3]) for row in RAW_DATA.itertuples(index=False)]\n",
    "TUPLES = list(set(TUPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fdf9c3-07e2-4912-8570-0614f7b9fdbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39392832594c4ce3ab88fcfe9d9cc475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing rich data from ommits:   0%|          | 0/763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Creates a lookup dictionary where any commit SHA can be looked up to grab the Commit object with all the data, + bag of paths\n",
    "COMMIT_DATA_LOOKUP = defaultdict(list)\n",
    "\n",
    "def _to_commit(pair):\n",
    "    sha = pair[0]\n",
    "    repo = pair[1]\n",
    "    commit = Commit(sha, f\"clones/{repo.replace('/', '-')}\")\n",
    "    commit._populate_commit_info()\n",
    "    commit._generate_bags_of_contexts()\n",
    "    \n",
    "    return commit\n",
    "\n",
    "resolved = []\n",
    "with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "    futures = [executor.submit(_to_commit, pair) for pair in TUPLES]\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing rich data from ommits\"):\n",
    "        resolved.append(future.result())\n",
    "\n",
    "for commit in tqdm(resolved, desc=\"Adding to lookup dictionary\"):\n",
    "    if result is not None:\n",
    "        COMMIT_DATA_LOOKUP[commit.sha] = commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6386a9f7-faae-4d6e-b441-6656c9c648e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Preprocess each commit metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5add242-33c0-4b00-afff-33485dfe6e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create X_Train and X_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f726de46-3e71-4172-8e3d-46fb4f4352e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Declare params for ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5f56908-0f3d-476e-aadb-2e4bdf6485ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f390403-cb08-48f1-9dec-693a128a0005",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Train SimSiam Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0b9318f-b5f4-40cb-bf1c-118f77d931ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Train binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b491ea8e-03e2-4408-9163-7078c3a9b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Evaluate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
