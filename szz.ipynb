{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c44c23b9-60df-4339-9ddd-b7808fb98f35",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GH Access Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c2410bd-ab7f-4cb0-9918-c774f3a22ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_token(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        token = f.read().strip()\n",
    "    return token\n",
    "\n",
    "GH_ACCESS_TOKEN = None #load_token(\"gh_access_token.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818e479d-9ae0-4728-a87e-8a30b2a7dc92",
   "metadata": {},
   "source": [
    "1. Log in to your GitHub account.\n",
    "2. Click on your profile picture in the top-right corner of the screen and select \"Settings\".\n",
    "3. In the left sidebar, click on \"Developer settings\".\n",
    "4. Click on \"Personal access tokens\".\n",
    "5. Click on \"Generate new token\".\n",
    "6. Give your token a description that will help you remember what it is used for.\n",
    "7. Select the permissions that your token requires. Note that you should only select the permissions that are necessary for your application to function.\n",
    "8. Click on \"Generate token\".\n",
    "9. Copy the generated access token.\n",
    "10. Open a text editor and create a new file called gh_access_token.txt.\n",
    "11. Paste the access token into the file.\n",
    "12. Save and close the file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b4efe-9400-4901-b189-9899fde73415",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Regular Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26140cd4-ed2b-40cd-80b3-efa7bbaae0d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import ast\n",
    "import git\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm, trange\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, wait, ALL_COMPLETED\n",
    "import networkx as nx\n",
    "import tree_sitter\n",
    "from tree_sitter import Language\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from github import Github\n",
    "import base64\n",
    "import cProfile\n",
    "import pstats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce25b8f6-e77c-46af-94c4-2d8eb95698c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Disable import warnings for tensorflow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['OMP_NUM_THREADS'] = '3'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10645da7-f5a0-4a21-81c5-564c37fffc49",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Machine Learning Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5af6eac8-dd15-4061-812c-e6d3fb862d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, LSTM, Attention, Concatenate, Layer,\n",
    "    Embedding, Dot, Softmax, TimeDistributed, Multiply,\n",
    "    Lambda, LayerNormalization, MultiHeadAttention,\n",
    "    Add, Masking, GlobalMaxPooling1D, GlobalMaxPooling2D, Reshape, MaxPooling1D, MaxPooling2D,\n",
    "    Dropout, Conv1D, Conv2D, Bidirectional, GRU, ConvLSTM2D, Flatten, Permute\n",
    ")\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import LambdaCallback, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MultiLabelBinarizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import MeanSquaredError, CosineSimilarity\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc241d3-5342-4b4c-8cd0-df6ad1410eab",
   "metadata": {},
   "source": [
    "### Data Pre-Processing Parameters\n",
    "These must be set before running the data preprocessing\n",
    "If any of these parameters are changed, all cells below **must** be re-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b27cdb5-80c9-4e0e-9e08-020b16d46caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of contexts to pad/reduce to per sample\n",
    "#This is the `k` number described in commit2seq\n",
    "BAG_SIZE = 32 # 256\n",
    "\n",
    "#Maximum depth of context\n",
    "#This is the depth of the path itself, so with a depth of 32, paths can be a maximum of 32 notes deep.\n",
    "#Smaller paths mean a higher focus on local dependencies, while longer paths represent a more general\n",
    "#representation of long-distance dependencies.\n",
    "CONTEXT_SIZE = 8 # 16\n",
    "\n",
    "#Internal fixed length representation size. This is the size of the fixed-length vector that the encoder\n",
    "#will learn to produce. Experimentation will be needed.\n",
    "OUTPUT_SIZE = 256 # 2048\n",
    "\n",
    "#Toggle to determine if tokens should be one-hot encoded or not\n",
    "ONE_HOT = False\n",
    "\n",
    "#Toogle to use root -> terminal paths instead of terminal -> terminal paths. \n",
    "#code2vec and commit2vec use terminal -> terminal paths, but generating them is significantly slower.\n",
    "ONLY_ROOT_PATHS = False\n",
    "\n",
    "#Toggle to drop paths above the context size limit. Likely should always be set to true.\n",
    "IGNORE_DEEP_PATHS = True\n",
    "\n",
    "#Generate full tuples of (terminal_token_a, (path...), terminal_token_b) instead of just paths.\n",
    "#commit2seq describes using this strategy as being quite a bit more effective (like 30%), but currently I\n",
    "#haven't figured out how to implement it.\n",
    "USE_FULL_TUPLES = False    #NOT working yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20803f20-9112-4fab-a6d6-b9a3879e7aa2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Token Vocab and Meta Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f25db691-0e1b-4a13-b172-fa0c69004123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load language files\n",
    "#The ast.so file must be re-generated to add support for more languages.\n",
    "#To do this, see: https://github.com/tree-sitter/py-tree-sitter\n",
    "\n",
    "JS_LANGUAGE = Language('ast-bindings/build/ast.so', 'javascript')\n",
    "PY_LANGUAGE = Language('ast-bindings/build/ast.so', 'python')\n",
    "JAVA_LANGUAGE = Language('ast-bindings/build/ast.so', 'java')\n",
    "C_LANGUAGE = Language('ast-bindings/build/ast.so', 'c')\n",
    "CPP_LANGUAGE = Language('ast-bindings/build/ast.so', 'cpp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5eb6c84-89d2-4798-aa86-3c6503bf17ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_json_data(lang):\n",
    "    url = f\"https://raw.githubusercontent.com/tree-sitter/tree-sitter-{lang}/master/src/node-types.json\"\n",
    "    data = json.loads(requests.get(url).text)\n",
    "    types = [node_type[\"type\"] for node_type in data]\n",
    "    for node_type in data:\n",
    "        if \"subtypes\" in node_type:\n",
    "            subtypes = [subtype[\"type\"] for subtype in node_type[\"subtypes\"]]\n",
    "            types.extend(subtypes)\n",
    "    types = list(set(types))\n",
    "    return types\n",
    "\n",
    "PYTHON_NODE_TYPES = download_json_data(\"python\")\n",
    "JAVA_NODE_TYPES = download_json_data(\"java\")\n",
    "JAVASCRIPT_NODE_TYPES = download_json_data(\"javascript\")\n",
    "C_NODE_TYPES = download_json_data(\"c\")\n",
    "CPP_NODE_TYPES = download_json_data(\"cpp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "248a6047-1fef-4c81-96d9-655140cc2a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For now set to just use JAVA node types. To enable support for other languages, just use a set union.\n",
    "ALL_NODE_TYPES = JAVA_NODE_TYPES\n",
    "#ALL_NODE_TYPES = list(set(python_data + java_data + javascript_data + c_data + cpp_data))\n",
    "\n",
    "# FILE_FILTERS = (\".c\", \".cpp\", \".java\", \".js\", \".py\")\n",
    "FILE_FILTERS = (\".java\")\n",
    "\n",
    "MAX_NODE_LOOKUP_NUM = len(ALL_NODE_TYPES)\n",
    "ALL_NODE_INDEXES = range(MAX_NODE_LOOKUP_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4366bc8c-97f9-43bf-898d-93ad374c28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automatically generated INPUT_SHAPE param. Don't modify\n",
    "if(ONE_HOT):\n",
    "    INPUT_SHAPE = (BAG_SIZE, CONTEXT_SIZE * (MAX_NODE_LOOKUP_NUM + 1))\n",
    "else:\n",
    "    INPUT_SHAPE = (BAG_SIZE, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db8769b-5427-4108-a6e9-c7bc94ecef34",
   "metadata": {},
   "source": [
    "### Load Raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c4aded3-0ed4-4dd6-8100-d40942c62f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA = pd.read_csv(\"pairs_apache_small.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37f1d401-c197-4a91-8228-fd243931f419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fix_hash</th>\n",
       "      <th>bug_hash</th>\n",
       "      <th>Y</th>\n",
       "      <th>bug_repo</th>\n",
       "      <th>fix_repo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0b05d3b939c5ed37a4253e7c3614d824e76ed664</td>\n",
       "      <td>0b92cec1e07a1f2d9aa70f3ecd7d0fb12290d2e2</td>\n",
       "      <td>1</td>\n",
       "      <td>apache/kafka</td>\n",
       "      <td>apache/kafka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f84bf9cbe771f252d5624e30d27755c9e5225179</td>\n",
       "      <td>d46e5634a3bca248e00b4f44e5216cd6607c5a52</td>\n",
       "      <td>1</td>\n",
       "      <td>apache/hbase</td>\n",
       "      <td>apache/hbase</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6c8f14b78afc0a6721433af6554b0ab45b8e163d</td>\n",
       "      <td>53989648276fc057d5ec7ab056a7d0a654d110b8</td>\n",
       "      <td>1</td>\n",
       "      <td>apache/hive</td>\n",
       "      <td>apache/hive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9623ae4de3e4c65ab2e90798843769795b8a00c6</td>\n",
       "      <td>505cf9d618c24f532e7b800d8b34a20a40c45feb</td>\n",
       "      <td>1</td>\n",
       "      <td>apache/kafka</td>\n",
       "      <td>apache/kafka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9fe8802cb83e05c0392b11b8dcfe354fecfda786</td>\n",
       "      <td>2e5c28f8c927f2caf6dcffc9070671089c5f771f</td>\n",
       "      <td>1</td>\n",
       "      <td>apache/hive</td>\n",
       "      <td>apache/hive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>16c934812b2395da7fb3968965b03d2f6aa8c8a3</td>\n",
       "      <td>0a08525ad236f78df05c854dead62f300eae271d</td>\n",
       "      <td>0</td>\n",
       "      <td>apache/cassandra</td>\n",
       "      <td>apache/cassandra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>b04bed022aec0f1f478a03383ab5184f048133b6</td>\n",
       "      <td>44f6c4b946511ce4663d41bf40f2960d2faee198</td>\n",
       "      <td>0</td>\n",
       "      <td>apache/kafka</td>\n",
       "      <td>apache/kafka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>b04bed022aec0f1f478a03383ab5184f048133b6</td>\n",
       "      <td>0d4cf64af359d22749f7e865c4efaee773d64962</td>\n",
       "      <td>0</td>\n",
       "      <td>apache/kafka</td>\n",
       "      <td>apache/kafka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>925599c72bc35edb5ffa25c6cc839932573b01aa</td>\n",
       "      <td>a8a73362bd9a3967c46011ded1ed831a586acd2e</td>\n",
       "      <td>0</td>\n",
       "      <td>apache/zookeeper</td>\n",
       "      <td>apache/zookeeper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>925599c72bc35edb5ffa25c6cc839932573b01aa</td>\n",
       "      <td>5a6535bf410ed223aac075241045ccd807b17b81</td>\n",
       "      <td>0</td>\n",
       "      <td>apache/zookeeper</td>\n",
       "      <td>apache/zookeeper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>882 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     fix_hash  \\\n",
       "0    0b05d3b939c5ed37a4253e7c3614d824e76ed664   \n",
       "1    f84bf9cbe771f252d5624e30d27755c9e5225179   \n",
       "2    6c8f14b78afc0a6721433af6554b0ab45b8e163d   \n",
       "3    9623ae4de3e4c65ab2e90798843769795b8a00c6   \n",
       "4    9fe8802cb83e05c0392b11b8dcfe354fecfda786   \n",
       "..                                        ...   \n",
       "877  16c934812b2395da7fb3968965b03d2f6aa8c8a3   \n",
       "878  b04bed022aec0f1f478a03383ab5184f048133b6   \n",
       "879  b04bed022aec0f1f478a03383ab5184f048133b6   \n",
       "880  925599c72bc35edb5ffa25c6cc839932573b01aa   \n",
       "881  925599c72bc35edb5ffa25c6cc839932573b01aa   \n",
       "\n",
       "                                     bug_hash  Y          bug_repo  \\\n",
       "0    0b92cec1e07a1f2d9aa70f3ecd7d0fb12290d2e2  1      apache/kafka   \n",
       "1    d46e5634a3bca248e00b4f44e5216cd6607c5a52  1      apache/hbase   \n",
       "2    53989648276fc057d5ec7ab056a7d0a654d110b8  1       apache/hive   \n",
       "3    505cf9d618c24f532e7b800d8b34a20a40c45feb  1      apache/kafka   \n",
       "4    2e5c28f8c927f2caf6dcffc9070671089c5f771f  1       apache/hive   \n",
       "..                                        ... ..               ...   \n",
       "877  0a08525ad236f78df05c854dead62f300eae271d  0  apache/cassandra   \n",
       "878  44f6c4b946511ce4663d41bf40f2960d2faee198  0      apache/kafka   \n",
       "879  0d4cf64af359d22749f7e865c4efaee773d64962  0      apache/kafka   \n",
       "880  a8a73362bd9a3967c46011ded1ed831a586acd2e  0  apache/zookeeper   \n",
       "881  5a6535bf410ed223aac075241045ccd807b17b81  0  apache/zookeeper   \n",
       "\n",
       "             fix_repo  \n",
       "0        apache/kafka  \n",
       "1        apache/hbase  \n",
       "2         apache/hive  \n",
       "3        apache/kafka  \n",
       "4         apache/hive  \n",
       "..                ...  \n",
       "877  apache/cassandra  \n",
       "878      apache/kafka  \n",
       "879      apache/kafka  \n",
       "880  apache/zookeeper  \n",
       "881  apache/zookeeper  \n",
       "\n",
       "[882 rows x 5 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAW_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81410104-2836-48e3-ae38-2e507197900d",
   "metadata": {},
   "source": [
    "### Load Rich Dataset from GH/git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7e58412-d70f-4531-bc94-275243a319ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d38dbb16a9461d82d03ba44e73b98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "REPO_LOOKUP = defaultdict(list)\n",
    "\n",
    "repos = set(list(RAW_DATA[\"bug_repo\"]) + list(RAW_DATA[\"fix_repo\"]))\n",
    "#Create a \"clones\" directory in order to clone local repos\n",
    "if not os.path.exists(\"clones\"):\n",
    "    os.makedirs(\"clones\")\n",
    "\n",
    "# Clone each \"repo\" into the \"clones\" folder\n",
    "for repo in tqdm(repos):\n",
    "    repo_folder = repo.replace(\"/\", \"-\")\n",
    "    if not os.path.exists(f\"clones/{repo_folder}\"):\n",
    "        os.system(f\"git clone https://github.com/{repo}.git clones/{repo_folder}\")\n",
    "    \n",
    "    #TODO: Try to implement this. Currently it breaks concurrant pre-processing\n",
    "    #REPO_LOOKUP[f\"clones/{repo_folder}\"] = git.Repo(f\"clones/{repo_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2233e5cb-ecd3-4139-a7f6-2070a9615e1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Commit Helper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1b64447-5f98-4b9c-9b7d-c87e063c27a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Commit:\n",
    "    \n",
    "    ###################################### SETUP ###################################### \n",
    "    \n",
    "    def __init__(self, sha, repo_path):\n",
    "        self.sha = sha\n",
    "        self.repo_path = repo_path\n",
    "        self.parent = None\n",
    "        self.author = None\n",
    "        self.datetime = None\n",
    "        self.message = None\n",
    "        self.bag_of_contexts = None\n",
    "        self._populate_commit_info()\n",
    "\n",
    "    def _populate_commit_info(self):\n",
    "        if(self.repo_path in REPO_LOOKUP):\n",
    "            repo = REPO_LOOKUP[self.repo_path]\n",
    "        else:\n",
    "            repo = git.Repo(self.repo_path)\n",
    "        commit = repo.commit(self.sha)\n",
    "        self.parent = commit.parents[0].hexsha if commit.parents else None\n",
    "        self.author = commit.author.name if commit.author else None\n",
    "        self.datetime = datetime.fromtimestamp(commit.committed_date)\n",
    "        self.message = commit.message if commit.message else None\n",
    "    \n",
    "    def _GH_populate_commit_info(self):\n",
    "        g = Github(GH_ACCESS_TOKEN)\n",
    "        repo = g.get_repo(self.repo_name)\n",
    "        commit = repo.get_commit(sha=self.sha)\n",
    "        self.parent = commit.parents[0].sha if commit.parents else None\n",
    "        self.author = commit.author.name if commit.author else None\n",
    "        self.datetime = commit.commit.author.date\n",
    "        self.message = commit.commit.message if commit.commit.message else None\n",
    "        \n",
    "    def _generate_bags_of_contexts(self):\n",
    "        self.bag_of_contexts = self.to_padded_consumable_data()\n",
    "\n",
    "    ###################################### GIT and GitHub ###################################### \n",
    "        \n",
    "    ################# GIT #################\n",
    "    \n",
    "    def get_files_changed(self):\n",
    "        try:\n",
    "            if(self.repo_path in REPO_LOOKUP):\n",
    "                repo = REPO_LOOKUP[self.repo_path]\n",
    "            else:\n",
    "                repo = git.Repo(self.repo_path)\n",
    "            commit = repo.commit(self.sha)\n",
    "            return [diff.a_path for diff in commit.diff(commit.parents[0]) if diff.a_path.endswith(FILE_FILTERS)]\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    def get_source_at_commit(self, file_name):\n",
    "        try:\n",
    "            if(self.repo_path in REPO_LOOKUP):\n",
    "                repo = REPO_LOOKUP[self.repo_path]\n",
    "            else:\n",
    "                repo = git.Repo(self.repo_path)\n",
    "            commit = repo.commit(self.sha)\n",
    "            return commit.tree[file_name].data_stream.read().decode('utf-8')\n",
    "        except:\n",
    "            return ''\n",
    "\n",
    "    def get_source_at_parent(self, file_name):\n",
    "        try:\n",
    "            if(self.repo_path in REPO_LOOKUP):\n",
    "                repo = REPO_LOOKUP[self.repo_path]\n",
    "            else:\n",
    "                repo = git.Repo(self.repo_path)\n",
    "            commit = repo.commit(self.sha)\n",
    "            return commit.parents[0].tree[file_name].data_stream.read().decode('utf-8')\n",
    "        except:\n",
    "            return ''\n",
    "    \n",
    "    ################# GitHub #################\n",
    "    #### Still todo, in testing ####\n",
    "    \n",
    "    def gh_get_files_changed(self):\n",
    "        try:\n",
    "            g = Github(GH_ACCESS_TOKEN)\n",
    "            repo = g.get_repo(self.repo_name)\n",
    "            commit = repo.get_commit(sha=self.sha)\n",
    "            return [f.filename for f in commit.files if f.filename.endswith(FILE_FILTERS)]\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    def gh_get_source_at_commit(self, file_name):\n",
    "        try:\n",
    "            g = Github(GH_ACCESS_TOKEN)\n",
    "            repo = g.get_repo(self.repo_name)\n",
    "            contents = repo.get_contents(file_name, ref=self.sha)\n",
    "            return base64.b64decode(contents.content).decode('utf-8')\n",
    "        except:\n",
    "            return ''\n",
    "\n",
    "    def gh_get_source_at_parent(self, file_name):\n",
    "        try:\n",
    "            g = Github(GH_ACCESS_TOKEN)\n",
    "            repo = g.get_repo(self.repo_name)\n",
    "            commit = repo.get_commit(sha=self.sha)\n",
    "            parent_commit = repo.get_commit(sha=commit.parents[0].sha)\n",
    "            contents = repo.get_contents(file_name, ref=parent_commit.sha)\n",
    "            return base64.b64decode(contents.content).decode('utf-8')\n",
    "        except:\n",
    "            return ''\n",
    "\n",
    "    ###################################### AST Processing ###################################### \n",
    "\n",
    "    def source_to_ast(self, source, file_name):\n",
    "        try:\n",
    "            parser = tree_sitter.Parser()\n",
    "            if file_name.endswith('.c'):\n",
    "                parser.set_language(C_LANGUAGE)\n",
    "            elif file_name.endswith('.cpp'):\n",
    "                parser.set_language(CPP_LANGUAGE)\n",
    "            elif file_name.endswith('.java'):\n",
    "                parser.set_language(JAVA_LANGUAGE)\n",
    "            elif file_name.endswith('.js'):\n",
    "                parser.set_language(JS_LANGUAGE)\n",
    "            elif file_name.endswith('.py'):\n",
    "                parser.set_language(PY_LANGUAGE)\n",
    "            else:\n",
    "                print(\"UNKNOWN LANGUAGE\")\n",
    "                return None\n",
    "            return parser.parse(bytes(source, 'utf8'))\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    ###################################### Bag of Contexts Processing ###################################### \n",
    "        \n",
    "    def get_root_paths(self, tree):\n",
    "        try:\n",
    "            paths = set()\n",
    "\n",
    "            # Recursive function to explore the tree\n",
    "            def explore(node, path, terminalA=None):\n",
    "                # Add current node to path\n",
    "                if terminalA is None:\n",
    "                    terminalA = node\n",
    "                path.append(node.type)\n",
    "\n",
    "                # If the node has no children, it's a leaf node and the path is complete\n",
    "                if not node.child_count:\n",
    "                    if USE_FULL_TUPLES:\n",
    "                        paths.add((terminalA.text, tuple(path), node.text))\n",
    "                    else:\n",
    "                        paths.add(tuple(path))\n",
    "                else:\n",
    "                    # Explore each child node recursively\n",
    "                    for child in node.children:\n",
    "                        explore(child, path, terminalA)\n",
    "\n",
    "                # Remove current node from path before returning\n",
    "                path.pop()\n",
    "\n",
    "            # Start exploring from the root node\n",
    "            root_node = tree.root_node\n",
    "            explore(root_node, [])\n",
    "            \n",
    "            return paths\n",
    "        except:\n",
    "            return set([])\n",
    "\n",
    "    def get_paths(self, ast):\n",
    "        graph = nx.Graph()\n",
    "        node_id = 0\n",
    "        leaf_nodes = []\n",
    "\n",
    "        def add_node(node):\n",
    "            nonlocal node_id\n",
    "            node_name = node.type\n",
    "            node_id += 1\n",
    "            graph.add_node(node_id, name=node_name, node=node, text=node.text)\n",
    "            if not node.children:\n",
    "                leaf_nodes.append(node_id)\n",
    "            return node_id\n",
    "\n",
    "        def add_edge(parent_id, child_id):\n",
    "            graph.add_edge(parent_id, child_id)\n",
    "\n",
    "        def traverse(node, parent_id=None):\n",
    "            current_id = add_node(node)\n",
    "            if parent_id is not None:\n",
    "                add_edge(parent_id, current_id)\n",
    "            for child in node.children:\n",
    "                traverse(child, current_id)\n",
    "\n",
    "        traverse(ast.root_node)\n",
    "\n",
    "        if BAG_SIZE != None:\n",
    "            leaf_nodes_sample = random.sample(leaf_nodes, min(BAG_SIZE, len(leaf_nodes)))\n",
    "        else:\n",
    "            leaf_nodes_sample = leaf_nodes\n",
    "        leaf_node_pairs = [(leaf_nodes_sample[i], leaf_nodes_sample[j]) for i in range(len(leaf_nodes_sample)) for j in range(i+1, len(leaf_nodes_sample))]\n",
    "        \n",
    "        if BAG_SIZE != None:\n",
    "            leaf_node_pairs = random.sample(leaf_node_pairs, min(BAG_SIZE,len(leaf_node_pairs)))\n",
    "        \n",
    "        all_paths = []\n",
    "        for pair in leaf_node_pairs:\n",
    "            path = list(nx.all_simple_paths(graph, source=pair[0], target=pair[1]))[0]\n",
    "            node_types = [graph.nodes[nodeID]['name'] for nodeID in path]\n",
    "            if(USE_FULL_TUPLES):\n",
    "                all_paths.append((graph.nodes[pair[0]]['text'], tuple(node_types), graph.nodes[pair[1]]['text']))\n",
    "            else:\n",
    "                all_paths.append(tuple(node_types))\n",
    "                \n",
    "        return set(all_paths)\n",
    "\n",
    "\n",
    "    def ast_to_bag_of_contexts(self, ast_trees):\n",
    "        paths = set()\n",
    "        for tree in ast_trees:\n",
    "            if(ONLY_ROOT_PATHS):\n",
    "                paths |= self.get_root_paths(tree)\n",
    "            else:\n",
    "                paths |= self.get_paths(tree)\n",
    "        return paths\n",
    "\n",
    "    ###################################### Padding and Encoding ###################################### \n",
    "    \n",
    "    def map_bag_of_contexts_to_id(self, bag_of_contexts):\n",
    "        mapped_paths = []\n",
    "        for path in bag_of_contexts:\n",
    "            mapped_path = []\n",
    "            for node in path:\n",
    "                index = ALL_NODE_TYPES.index(node)\n",
    "                mapped_path.append(index + 1)\n",
    "            mapped_paths.append(mapped_path)\n",
    "        return mapped_paths\n",
    "\n",
    "    def one_hot_encode(self, bag_of_contexts):\n",
    "        one_hot_paths = []\n",
    "\n",
    "        # Iterate over each row in the array\n",
    "        for row in bag_of_contexts:\n",
    "            # Create an empty list to hold the one-hot encodings for this row\n",
    "            row_one_hot = []\n",
    "\n",
    "            # Iterate over each element in the row\n",
    "            for num in row:\n",
    "                # Create an empty list to hold the one-hot encoding for this number\n",
    "                num_one_hot = [0] * (MAX_NODE_LOOKUP_NUM + 1)\n",
    "\n",
    "                # Set the corresponding element to 1\n",
    "                num_one_hot[int(num)] = 1\n",
    "\n",
    "                # Add the one-hot encoding for this number to the row's list\n",
    "                row_one_hot.append(num_one_hot)\n",
    "\n",
    "            # Add the row's list of one-hot encodings to the main list\n",
    "            one_hot_paths.append(row_one_hot)\n",
    "\n",
    "        return one_hot_paths\n",
    "\n",
    "    def pad_each_context(self, bag_of_contexts):\n",
    "        padded_one_hot_paths = []\n",
    "        for path in bag_of_contexts:\n",
    "            if IGNORE_DEEP_PATHS and len(path) > CONTEXT_SIZE:\n",
    "                continue\n",
    "            if ONE_HOT:\n",
    "                padded_path = [[0] * (MAX_NODE_LOOKUP_NUM + 1)] * max(CONTEXT_SIZE - len(path), 0) + path[-CONTEXT_SIZE:]\n",
    "            else:\n",
    "                padded_path = [0] * max(CONTEXT_SIZE - len(path), 0) + path[-CONTEXT_SIZE:]\n",
    "            padded_one_hot_paths.append(padded_path)\n",
    "        return padded_one_hot_paths\n",
    "\n",
    "    ###################################### Utility ###################################### \n",
    "    \n",
    "    def to_raw_consumable_data(self):\n",
    "        files_changed = self.get_files_changed()[:32]\n",
    "        sources_at_commit = [self.get_source_at_commit(filename) for filename in files_changed]\n",
    "        sources_at_parent = [self.get_source_at_parent(filename) for filename in files_changed]        \n",
    "\n",
    "        asts_commit = [self.source_to_ast(source, files_changed[i]) for i, source in enumerate(sources_at_commit)]\n",
    "        asts_parent = [self.source_to_ast(source, files_changed[i]) for i, source in enumerate(sources_at_parent)]\n",
    "        \n",
    "        contexts_commit = self.ast_to_bag_of_contexts(asts_commit)\n",
    "        contexts_parent = self.ast_to_bag_of_contexts(asts_parent)\n",
    "\n",
    "        contexts = list(contexts_commit.symmetric_difference(contexts_parent))\n",
    "\n",
    "        \n",
    "        contexts = self.map_bag_of_contexts_to_id(contexts)\n",
    "\n",
    "        if(ONE_HOT):\n",
    "            contexts = self.one_hot_encode(contexts)\n",
    "\n",
    "        contexts = self.pad_each_context(contexts)\n",
    "\n",
    "        return contexts\n",
    "    \n",
    "    def raw_to_padded(self, consumable):\n",
    "        if(len(consumable) == BAG_SIZE):\n",
    "            return consumable\n",
    "        \n",
    "        if(len(consumable) > BAG_SIZE):\n",
    "            return random.sample(consumable, BAG_SIZE)\n",
    "        \n",
    "        if ONE_HOT:\n",
    "            blank_path = [[0] * (MAX_NODE_LOOKUP_NUM + 1)] * CONTEXT_SIZE\n",
    "        else:\n",
    "            blank_path = ([0] * CONTEXT_SIZE)\n",
    "            \n",
    "        return ([blank_path] * (BAG_SIZE - len(consumable)) + consumable)\n",
    "\n",
    "    \n",
    "    def to_padded_consumable_data(self):\n",
    "        consumable = self.to_raw_consumable_data()\n",
    "        padded = self.raw_to_padded(consumable)\n",
    "        if(ONE_HOT):\n",
    "            padded = np.array(padded)\n",
    "            padded = padded.reshape(padded.shape[0], padded.shape[1] * padded.shape[2])\n",
    "            padded = padded.tolist()\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fddd13-51c1-4555-a3a2-0bf977af7486",
   "metadata": {},
   "source": [
    "#### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d7d0781-6305-4bd4-873c-0b6979a4db19",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA = pd.concat([RAW_DATA.iloc[:32], RAW_DATA.iloc[-32:]])\n",
    "TUPLES = [(row[0], row[4]) for i, row in RAW_DATA.iterrows()] + [(row[1], row[3]) for i, row in RAW_DATA.iterrows()]\n",
    "TUPLES = list(set(TUPLES))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe6f6ae-39f6-48df-840c-233d1b83199e",
   "metadata": {},
   "source": [
    "#### Note: This takes a long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0fdf9c3-07e2-4912-8570-0614f7b9fdbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56e7d4fcf6c043ff961d3b8e68334ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing rich data from ommits:   0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHA b'eb91c4c03e8400811f7291294b6132f9a22a4d3f' could not be resolved, git returned: b'eb91c4c03e8400811f7291294b6132f9a22a4d3f missing'\n",
      "SHA b'a8abd6b023e98903e9f200c36173f23a8022f907' could not be resolved, git returned: b'a8abd6b023e98903e9f200c36173f23a8022f907 missing'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f022c8cdb0a145e799dd937844b06e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding to lookup dictionary:   0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Creates a lookup dictionary where any commit SHA can be looked up to grab the Commit object with all the data, + bag of paths\n",
    "COMMIT_DATA_LOOKUP = defaultdict(list)\n",
    "\n",
    "def _to_commit(pair):\n",
    "    sha = pair[0]\n",
    "    repo = pair[1]\n",
    "    commit = Commit(sha, f\"clones/{repo.replace('/', '-')}\")\n",
    "    commit._populate_commit_info()\n",
    "    commit._generate_bags_of_contexts()\n",
    "    \n",
    "    return commit\n",
    "\n",
    "resolved = []\n",
    "with ThreadPoolExecutor(max_workers=16) as executor:\n",
    "    futures = [executor.submit(_to_commit, pair) for pair in TUPLES]\n",
    "    for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing rich data from commits\"):\n",
    "        try:\n",
    "            resolved.append(future.result())\n",
    "        except Exception as e:\n",
    "            resolved.append(None)\n",
    "            print(e)\n",
    "\n",
    "for commit in tqdm(resolved, desc=\"Adding to lookup dictionary\"):\n",
    "    if commit is not None:\n",
    "        COMMIT_DATA_LOOKUP[commit.sha] = commit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92a588e-ff4e-4566-ac72-068042d00a0b",
   "metadata": {},
   "source": [
    "### Metadata processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1911ee7d-c716-4951-8164-3cc58ff289e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processNames(name1: str, name2: str):\n",
    "    if name1 == name2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Process two timestamnps and return their positive difference\n",
    "def processTimestamps(timestamp1: int, timestamp2: int):\n",
    "    difference = timestamp1 - timestamp2\n",
    "    if(difference < 0):\n",
    "        difference *= -1\n",
    "    return difference\n",
    "\n",
    "# Process two commit messages and return a similarity score from 0 to 1\n",
    "def processCommitMessages(commit1: str, commit2: str):\n",
    "    similarity = cosineSimilarity(commit1, commit2)\n",
    "    return similarity\n",
    "\n",
    "\n",
    "# Calculate the cosine similarity of two strings\n",
    "def cosineSimilarity(str1: str, str2: str):\n",
    "    vectors = sentences2Vecs([str1, str2])\n",
    "    similarity = cosine_similarity(vectors[0].reshape(1, -1), vectors[1].reshape(1,-1))[0][0]\n",
    "    normalized_similarity = round((similarity + 1) / 2, 6)\n",
    "    return normalized_similarity\n",
    "\n",
    "# Turn an array of sentences into an array of vectors using a special sentence transformer using their paraphrase-MiniLM-L6-v2 model for relatively quick performance.  \n",
    "# In my small amount of testing, takes about 4 seconds to load the model the first time and perform the transformations, and then 0.3 secconds after that\n",
    "def sentences2Vecs(sentences):\n",
    "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f5fdb-78e5-4f94-8a10-e795c0e9a819",
   "metadata": {},
   "source": [
    "### Dataset finalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "023a6134-55ed-472f-bcc5-b23159c286a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c887132df73c499ca75564afc7ddb6b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating X_train:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = [\n",
    "    (\n",
    "        np.array(processNames(COMMIT_DATA_LOOKUP[row[0]].author, COMMIT_DATA_LOOKUP[row[1]].author)),\n",
    "        np.array(processTimestamps(COMMIT_DATA_LOOKUP[row[0]].datetime.timestamp(), COMMIT_DATA_LOOKUP[row[1]].datetime.timestamp())),\n",
    "        np.array(processCommitMessages(COMMIT_DATA_LOOKUP[row[0]].message, COMMIT_DATA_LOOKUP[row[1]].message)),\n",
    "        np.array(COMMIT_DATA_LOOKUP[row[0]].bag_of_contexts),\n",
    "        np.array(COMMIT_DATA_LOOKUP[row[1]].bag_of_contexts)\n",
    "    )\n",
    "    for i, row in tqdm(RAW_DATA.iterrows(), total=len(RAW_DATA), desc=\"Generating X_train\") if row[0] in COMMIT_DATA_LOOKUP and row[1] in COMMIT_DATA_LOOKUP\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bcff4092-1693-44ca-ae31-d4d13ed268bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = [row[2] for i, row in RAW_DATA.iterrows() if row[0] in COMMIT_DATA_LOOKUP and row[1] in COMMIT_DATA_LOOKUP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f40b2a56-53d5-4366-84d1-37311d76ba76",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_BAGS_OF_CONTEXTS = np.array([COMMIT_DATA_LOOKUP[sha].bag_of_contexts for sha in COMMIT_DATA_LOOKUP])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a5add242-33c0-4b00-afff-33485dfe6e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9a4e0ba4-132b-47f2-9088-71d66e7ebfd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5, 50)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_train) / len(y_train), len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a9010d-c555-4ef9-b4b2-6f9bad7c8101",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "62ca5aa9-fa88-4c00-af44-7b6e0695837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommitDiffModel:\n",
    "    def __init__(self):\n",
    "        self.input_shape = INPUT_SHAPE\n",
    "        self.example_size = BAG_SIZE\n",
    "        self.context_size = CONTEXT_SIZE\n",
    "        self.fixed_vector_size = OUTPUT_SIZE\n",
    "        self.num_heads = 4\n",
    "        self.key_dim = 512\n",
    "        self.units = 128\n",
    "        self.rate = 0.1\n",
    "        self.activation_fn1 = \"relu\"\n",
    "        self.activation_fn2 = \"relu\"\n",
    "        self.activation_fn3 = \"relu\"\n",
    "        self.optimizer = \"adam\"\n",
    "        self.loss_fn = \"mse\"\n",
    "        self.temperature = 0.1\n",
    "        self.encoder = None\n",
    "        self.siam_model = None\n",
    "        self.binary_classification_model = None\n",
    "\n",
    "    def initialize(self):\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.siam_model = self.build_siam_model()\n",
    "        self.binary_classification_model = self.build_binary_classification_model()\n",
    "        \n",
    "    def encoder_attention_feed_forward(self, inputs):\n",
    "        \n",
    "        inputs = Reshape((self.example_size, -1))(inputs)\n",
    "        \n",
    "        # Self-attention layer to obtain attention weights\n",
    "        self_attention = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.key_dim)(inputs, inputs)\n",
    "        \n",
    "        normalized = LayerNormalization()(self_attention)\n",
    "        \n",
    "        # Feedforward layers\n",
    "        feedforward = TimeDistributed(Dense(units=self.units, activation=self.activation_fn1))(normalized)\n",
    "\n",
    "        # Global max pooling layer to produce a fixed-length output vector representation\n",
    "        pooled = MaxPooling1D(pool_size=self.context_size)(feedforward)\n",
    "\n",
    "        return pooled\n",
    "    \n",
    "    def encoder_attention_weighted(self, inputs):\n",
    "        \n",
    "        # Add a self-attention layer to compute attention weights for each context\n",
    "        self_attention = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.key_dim)(inputs, inputs)\n",
    "\n",
    "        # Add a layer normalization layer to normalize the self-attention output\n",
    "        normalized = LayerNormalization()(self_attention)\n",
    "\n",
    "        # Add a feedforward layer to compute context-level encodings\n",
    "        encoded = Dense(units=self.units, activation='relu')(normalized)\n",
    "\n",
    "        # Compute attention weights based on the contents of each context\n",
    "        attention_weights = Dense(units=1, activation='softmax')(encoded)\n",
    "\n",
    "        # Compute a weighted sum of the contexts using attention weights\n",
    "        weighted_sum = tf.keras.layers.Dot(axes=1)([attention_weights, encoded])\n",
    "        \n",
    "        return weighted_sum\n",
    "    \n",
    "    \n",
    "    #NOTE: Conceptually good, but doesn't actually compile :(\n",
    "    def encoder_softmax_pooled(self, inputs):\n",
    "        \n",
    "        # Self-attention layer to obtain attention weights\n",
    "        self_attention = MultiHeadAttention(num_heads=self.num_heads, key_dim=self.key_dim)(inputs, inputs)\n",
    "\n",
    "        # Compute attention weights and apply them to the inputs\n",
    "        attention_weights = Softmax()(self_attention)\n",
    "        weighted_inputs = Multiply()([attention_weights, inputs])\n",
    "\n",
    "        # Sum pooling to aggregate the weighted bag\n",
    "        pooled_inputs = Lambda(lambda x: K.sum(x, axis=1))(weighted_inputs)\n",
    "\n",
    "        normalized = LayerNormalization()(pooled_inputs)\n",
    "        \n",
    "        # Feedforward layers\n",
    "        feedforward = TimeDistributed(Dense(units=self.units, activation=self.activation_fn1))(normalized)\n",
    "\n",
    "        # Global max pooling layer to produce a fixed-length output vector representation\n",
    "        pooled = MaxPooling1D(pool_size=self.context_size)(feedforward)\n",
    "\n",
    "        return pooled\n",
    "    \n",
    "    #Best so far. Haven't played with hyperparams\n",
    "    def encoder_recurrent_convolutional(self, inputs):\n",
    "       \n",
    "        # Add a 1D convolutional layer to extract features from each context\n",
    "        conv = Conv1D(filters=32, kernel_size=3, activation='relu')(inputs)\n",
    "\n",
    "        # Add a max pooling layer to summarize the extracted features\n",
    "        max_pooling = MaxPooling1D(pool_size=self.context_size)(conv)\n",
    "\n",
    "        # Add a recurrent layer to capture temporal dependencies within each context\n",
    "        lstm = LSTM(units=64)(max_pooling)\n",
    "\n",
    "        return lstm\n",
    "    \n",
    "    def encoder_hierarchical(self, inputs):\n",
    "        \n",
    "        # Encode each individual context using a bidirectional LSTM layer\n",
    "        encoded_contexts = Bidirectional(LSTM(units=self.units, return_sequences=True))(inputs)\n",
    "\n",
    "        # Compute a weighted sum of the context-level encodings, using attention weights based on the contents of each context\n",
    "        attention_weights = TimeDistributed(Dense(units=1, activation='softmax'))(encoded_contexts)\n",
    "        weighted_sum = tf.keras.layers.Dot(axes=1)([attention_weights, encoded_contexts])\n",
    "\n",
    "        return weighted_sum\n",
    "        \n",
    "    def build_encoder(self):\n",
    "        \n",
    "        inputs = Input(shape=self.input_shape)\n",
    "\n",
    "        masked_inputs = Masking()(inputs)\n",
    "        \n",
    "        ##########################################################\n",
    "        \n",
    "        # encoded = self.encoder_attention_feed_forward(masked_inputs)\n",
    "        # encoded = self.encoder_softmax_pooled(masked_inputs) #Broken\n",
    "        encoded = self.encoder_recurrent_convolutional(masked_inputs) #Best so far with small inputs\n",
    "        # encoded = self.encoder_attention_weighted(masked_inputs)\n",
    "        # encoded = self.encoder_hierarchical(masked_inputs)\n",
    "        \n",
    "        ##########################################################\n",
    "\n",
    "        # Dropout layer to prevent overfitting\n",
    "        dropout = Dropout(rate=self.rate)(encoded)\n",
    "\n",
    "        # Fixed length vector representation\n",
    "        outputs = Dense(units=self.fixed_vector_size)(dropout)\n",
    "\n",
    "        # Define encoder model\n",
    "        encoder = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        return encoder\n",
    "    \n",
    "    def build_siam_model(self):\n",
    "        \n",
    "        # Create SimSiam model\n",
    "        x1 = Input(shape=self.input_shape)\n",
    "        x2 = Input(shape=self.input_shape)\n",
    "        \n",
    "        x1 = tf.random.shuffle(x1)\n",
    "        x2 = tf.random.shuffle(x2)\n",
    "        \n",
    "        # Encode the input twice using the same encoder\n",
    "        z1 = self.encoder(x1)\n",
    "        z2 = self.encoder(x2)\n",
    "        \n",
    "        # Predict a transformation of the first encoding\n",
    "        p1 = Dense(units=self.fixed_vector_size, activation=self.activation_fn2)(z1)\n",
    "        p2 = Dense(units=self.fixed_vector_size, activation=self.activation_fn2)(z1)\n",
    "        \n",
    "        #Loss function\n",
    "        def D(p, z):\n",
    "            z = tf.stop_gradient(z)\n",
    "            p = tf.math.l2_normalize(p, axis=1)\n",
    "            z = tf.math.l2_normalize(z, axis=1)\n",
    "            return -tf.reduce_mean(tf.reduce_sum(p * z, axis=1))\n",
    "        \n",
    "        loss = D(p1, z2) / 2 + D(p2, z1) / 2\n",
    "        \n",
    "        # Define the model\n",
    "        model = tf.keras.Model(inputs=[x1,x2], outputs=loss)\n",
    "        \n",
    "        # Compile the model\n",
    "        model.compile(optimizer=self.optimizer, loss=lambda _, loss: loss)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def build_binary_classification_model(self):\n",
    "        \n",
    "        # Create binary classification model\n",
    "        name_input = Input(shape=())\n",
    "        timestamp_input = Input(shape=())\n",
    "        message_input = Input(shape=())\n",
    "        bag1_input = Input(shape=self.input_shape)\n",
    "        bag2_input = Input(shape=self.input_shape)\n",
    "        \n",
    "        # Encode bag of contexts using the same encoder model\n",
    "        encoded1 = self.encoder(bag1_input)\n",
    "        encoded2 = self.encoder(bag2_input)\n",
    "        for layer in self.encoder.layers:\n",
    "            layer.trainable = False\n",
    "            \n",
    "        name_reshaped = Reshape((1,))(name_input)\n",
    "        timestamp_reshaped = Reshape((1,))(timestamp_input)\n",
    "        message_reshaped = Reshape((1,))(message_input)\n",
    "\n",
    "        \n",
    "        merged = Concatenate()([encoded1, encoded2, name_reshaped, timestamp_reshaped, message_reshaped])\n",
    "        \n",
    "        # Binary classification output\n",
    "        binary_classification = Dense(units=512, activation=self.activation_fn3)(merged)\n",
    "        binary_classification = Dense(units=512, activation=self.activation_fn3)(binary_classification)\n",
    "        binary_classification = Dense(units=256, activation=self.activation_fn3)(binary_classification)\n",
    "        binary_classification = Dense(units=1, activation=self.activation_fn3)(binary_classification)\n",
    "        \n",
    "        # Define model\n",
    "        model = tf.keras.Model(inputs=[name_input, timestamp_input, message_input, bag1_input, bag2_input], outputs=binary_classification)\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(optimizer=self.optimizer, loss=\"mse\")\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def fit_siam(self, X_train, epochs, batch_size, verbose=0):        \n",
    "        self.siam_model.fit([X_train, X_train], [X_train, X_train], epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "        \n",
    "    def fit_binary_classification(self, X_train, y_train, epochs, batch_size, verbose=0):\n",
    "        self.binary_classification_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "\n",
    "    def evaluate_binary_classification(self, X_test, y_test, verbose=0):\n",
    "        return self.binary_classification_model.evaluate(X_test, y_test, verbose=verbose)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f726de46-3e71-4172-8e3d-46fb4f4352e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CommitDiffModel()\n",
    "#TODO: Modify ML model hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a5f56908-0f3d-476e-aadb-2e4bdf6485ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faeb7f5-8211-45ee-aed0-10539860b7e1",
   "metadata": {},
   "source": [
    "### Model Unsupervised Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6f390403-cb08-48f1-9dec-693a128a0005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "14/14 [==============================] - 3s 5ms/step - loss: -0.3427\n",
      "Epoch 2/8\n",
      "14/14 [==============================] - 0s 6ms/step - loss: -0.5207\n",
      "Epoch 3/8\n",
      "14/14 [==============================] - 0s 7ms/step - loss: -0.5782\n",
      "Epoch 4/8\n",
      "14/14 [==============================] - 0s 5ms/step - loss: -0.6293\n",
      "Epoch 5/8\n",
      "14/14 [==============================] - 0s 7ms/step - loss: -0.6510\n",
      "Epoch 6/8\n",
      "14/14 [==============================] - 0s 5ms/step - loss: -0.6610\n",
      "Epoch 7/8\n",
      "14/14 [==============================] - 0s 7ms/step - loss: -0.6726\n",
      "Epoch 8/8\n",
      "14/14 [==============================] - 0s 6ms/step - loss: -0.6835\n"
     ]
    }
   ],
   "source": [
    "model.fit_siam(np.array(ALL_BAGS_OF_CONTEXTS), epochs=8, batch_size=8, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe4e68c-53ab-4e57-8d0a-3df0cbd06e10",
   "metadata": {},
   "source": [
    "### Model Supervised Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c0b9318f-b5f4-40cb-bf1c-118f77d931ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_binary_classification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[68], line 211\u001b[0m, in \u001b[0;36mCommitDiffModel.fit_binary_classification\u001b[0;34m(self, X_train, y_train, epochs, batch_size, verbose)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_binary_classification\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_train, y_train, epochs, batch_size, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_classification_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:906\u001b[0m, in \u001b[0;36mTensorShape.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    905\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_v2_behavior:\n\u001b[0;32m--> 906\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dims\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    907\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims[key]\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "model.fit_binary_classification(X_train, np.array(y_train), epochs=4, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b491ea8e-03e2-4408-9163-7078c3a9b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate_binary_classification(X_test, np.array(y_test), verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d5a3fc-8a45-4aa4-9924-e270bcb7375d",
   "metadata": {},
   "source": [
    "### Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae44bb33-d37b-4441-afa5-b019601dc6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model.siam_model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975e0b6d-5923-4e6c-a27a-0e667bc4770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model.encoder, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e66c847-e578-4fc4-bcfe-998796423035",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model.binary_classification_model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a93e65e-0ea4-4503-b47c-28b2c0e8e24c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
