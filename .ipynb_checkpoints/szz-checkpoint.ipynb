{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c44c23b9-60df-4339-9ddd-b7808fb98f35",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Regular Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "26140cd4-ed2b-40cd-80b3-efa7bbaae0d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import ast\n",
    "import git\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm.auto import tqdm, trange\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, wait, ALL_COMPLETED\n",
    "import networkx as nx\n",
    "import tree_sitter\n",
    "from tree_sitter import Language\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce25b8f6-e77c-46af-94c4-2d8eb95698c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Disable import warnings for tensorflow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10645da7-f5a0-4a21-81c5-564c37fffc49",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Machine Learning Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5af6eac8-dd15-4061-812c-e6d3fb862d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, LSTM, Attention, Concatenate, Layer,\n",
    "    Embedding, Dot, Softmax, TimeDistributed, Multiply,\n",
    "    Lambda, LayerNormalization, MultiHeadAttention,\n",
    "    Add, Masking, GlobalMaxPooling1D, GlobalMaxPooling2D, Reshape, MaxPooling1D, MaxPooling2D,\n",
    "    Dropout, Conv1D, Conv2D, Bidirectional, GRU, ConvLSTM2D, Flatten, Permute\n",
    ")\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import LambdaCallback, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MultiLabelBinarizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import MeanSquaredError, CosineSimilarity\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc241d3-5342-4b4c-8cd0-df6ad1410eab",
   "metadata": {},
   "source": [
    "### Data Pre-Processing Parameters\n",
    "These must be set before running the data preprocessing\n",
    "If any of these parameters are changed, all cells below **must** be re-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8b27cdb5-80c9-4e0e-9e08-020b16d46caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of contexts to pad/reduce to per sample\n",
    "#This is the `k` number described in commit2seq\n",
    "BAG_SIZE = 256\n",
    "\n",
    "#Maximum depth of context\n",
    "#This is the depth of the path itself, so with a depth of 32, paths can be a maximum of 32 notes deep.\n",
    "#Smaller paths mean a higher focus on local dependencies, while longer paths represent a more general\n",
    "#representation of long-distance dependencies.\n",
    "CONTEXT_SIZE = 16\n",
    "\n",
    "#Internal fixed length representation size. This is the size of the fixed-length vector that the encoder\n",
    "#will learn to produce. Experimentation will be needed.\n",
    "OUTPUT_SIZE = 2048\n",
    "\n",
    "#Toggle to determine if tokens should be one-hot encoded or not\n",
    "ONE_HOT = False\n",
    "\n",
    "#Toogle to use root -> terminal paths instead of terminal -> terminal paths. \n",
    "#code2vec and commit2vec use terminal -> terminal paths, but generating them is significantly slower.\n",
    "ONLY_ROOT_PATHS = False\n",
    "\n",
    "#Toggle to drop paths above the context size limit. Likely should always be set to true.\n",
    "IGNORE_DEEP_PATHS = True\n",
    "\n",
    "#Generate full tuples of (terminal_token_a, (path...), terminal_token_b) instead of just paths.\n",
    "#commit2seq describes using this strategy as being quite a bit more effective (like 30%), but currently I\n",
    "#haven't figured out how to implement it.\n",
    "USE_FULL_TUPLES = False    #NOT working yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20803f20-9112-4fab-a6d6-b9a3879e7aa2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Token Vocab and Meta Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f25db691-0e1b-4a13-b172-fa0c69004123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load language files\n",
    "#The ast.so file must be re-generated to add support for more languages.\n",
    "#To do this, see: https://github.com/tree-sitter/py-tree-sitter\n",
    "\n",
    "JS_LANGUAGE = Language('ast-bindings/build/ast.so', 'javascript')\n",
    "PY_LANGUAGE = Language('ast-bindings/build/ast.so', 'python')\n",
    "JAVA_LANGUAGE = Language('ast-bindings/build/ast.so', 'java')\n",
    "C_LANGUAGE = Language('ast-bindings/build/ast.so', 'c')\n",
    "CPP_LANGUAGE = Language('ast-bindings/build/ast.so', 'cpp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f5eb6c84-89d2-4798-aa86-3c6503bf17ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download_json_data(lang):\n",
    "    url = f\"https://raw.githubusercontent.com/tree-sitter/tree-sitter-{lang}/master/src/node-types.json\"\n",
    "    data = json.loads(requests.get(url).text)\n",
    "    types = [node_type[\"type\"] for node_type in data]\n",
    "    for node_type in data:\n",
    "        if \"subtypes\" in node_type:\n",
    "            subtypes = [subtype[\"type\"] for subtype in node_type[\"subtypes\"]]\n",
    "            types.extend(subtypes)\n",
    "    types = list(set(types))\n",
    "    return types\n",
    "\n",
    "PYTHON_NODE_TYPES = download_json_data(\"python\")\n",
    "JAVA_NODE_TYPES = download_json_data(\"java\")\n",
    "JAVASCRIPT_NODE_TYPES = download_json_data(\"javascript\")\n",
    "C_NODE_TYPES = download_json_data(\"c\")\n",
    "CPP_NODE_TYPES = download_json_data(\"cpp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "248a6047-1fef-4c81-96d9-655140cc2a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For now set to just use JAVA node types. To enable support for other languages, just use a set union.\n",
    "ALL_NODE_TYPES = JAVA_NODE_TYPES\n",
    "#ALL_NODE_TYPES = list(set(python_data + java_data + javascript_data + c_data + cpp_data))\n",
    "\n",
    "MAX_NODE_LOOKUP_NUM = len(ALL_NODE_TYPES)\n",
    "ALL_NODE_INDEXES = range(MAX_NODE_LOOKUP_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4366bc8c-97f9-43bf-898d-93ad374c28ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Automatically generated INPUT_SHAPE param. Don't modify\n",
    "if(ONE_HOT):\n",
    "    INPUT_SHAPE = (EXAMPLE_SIZE, CONTEXT_SIZE * (MAX_NODE_LOOKUP_NUM + 1))\n",
    "else:\n",
    "    INPUT_SHAPE = (EXAMPLE_SIZE, CONTEXT_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db8769b-5427-4108-a6e9-c7bc94ecef34",
   "metadata": {},
   "source": [
    "### Load Raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7c4aded3-0ed4-4dd6-8100-d40942c62f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA = pd.read_csv(\"pairs_apache_small.csv\")[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "37f1d401-c197-4a91-8228-fd243931f419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>fix_hash</th>\n",
       "      <th>bug_hash</th>\n",
       "      <th>repo</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>304.0</td>\n",
       "      <td>ea52d836b45cb1c77e243b906172738e3a7fa587</td>\n",
       "      <td>24b065cc91f7bcdab25fc3634699657ac2f27104</td>\n",
       "      <td>apache/hbase</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>340.0</td>\n",
       "      <td>bb8e346466c6f38d8cf11282513bbc139bddf88e</td>\n",
       "      <td>9dbddf3da40e30a3658a60bde2337bf3a2f8207c</td>\n",
       "      <td>apache/hbase</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47.0</td>\n",
       "      <td>4008068ad83063c0f7017dc6c1cb5a718caaf7b7</td>\n",
       "      <td>47b898d0376f5d613a4233257b1eef918a6c2122</td>\n",
       "      <td>apache/hbase</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                  fix_hash  \\\n",
       "0  304.0  ea52d836b45cb1c77e243b906172738e3a7fa587   \n",
       "1  340.0  bb8e346466c6f38d8cf11282513bbc139bddf88e   \n",
       "2   47.0  4008068ad83063c0f7017dc6c1cb5a718caaf7b7   \n",
       "\n",
       "                                   bug_hash          repo  Y  \n",
       "0  24b065cc91f7bcdab25fc3634699657ac2f27104  apache/hbase  1  \n",
       "1  9dbddf3da40e30a3658a60bde2337bf3a2f8207c  apache/hbase  1  \n",
       "2  47b898d0376f5d613a4233257b1eef918a6c2122  apache/hbase  1  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RAW_DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81410104-2836-48e3-ae38-2e507197900d",
   "metadata": {},
   "source": [
    "### Load Rich Dataset from GH/git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b7e58412-d70f-4531-bc94-275243a319ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85a0ef5f42d1452a9963eaa85f7a82b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "repos = set(RAW_DATA[\"repo\"])\n",
    "repos = set([\"apache/hbase\"])\n",
    "\n",
    "#Create a \"clones\" directory in order to clone local repos\n",
    "if not os.path.exists(\"clones\"):\n",
    "    os.makedirs(\"clones\")\n",
    "\n",
    "# Clone each \"repo\" into the \"clones\" folder\n",
    "for repo in tqdm(repos):\n",
    "    repo_folder = repo.replace(\"/\", \"-\")\n",
    "    if not os.path.exists(repo_path):\n",
    "        os.system(f\"git clone https://github.com/{repo}.git clones/{repo_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b1b64447-5f98-4b9c-9b7d-c87e063c27a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Commit:\n",
    "    def __init__(self, sha, repo_path):\n",
    "        self.sha = sha\n",
    "        self.repo_path = repo_path\n",
    "        self.parent = None\n",
    "        self.author = None\n",
    "        self.datetime = None\n",
    "        self.message = None\n",
    "        self.bag_of_contexts = None\n",
    "        self._populate_commit_info()\n",
    "\n",
    "    def _populate_commit_info(self):\n",
    "        repo = git.Repo(self.repo_path)\n",
    "        commit = repo.commit(self.sha)\n",
    "        self.parent = commit.parents[0].hexsha if commit.parents else None\n",
    "        self.author = commit.author.name if commit.author else None\n",
    "        self.datetime = datetime.fromtimestamp(commit.committed_date)\n",
    "        self.message = commit.message if commit.message else None\n",
    "        \n",
    "    def _generate_bags_of_contexts(self):\n",
    "        self.bag_of_contexts = self.to_padded_consumable_data()\n",
    "        \n",
    "    def get_files_changed(self, filters = (\".c\", \".cpp\", \".java\", \".js\", \".py\")):\n",
    "        try:\n",
    "            repo = git.Repo(self.repo_path)\n",
    "            commit = repo.commit(self.sha)\n",
    "            return [diff.a_path for diff in commit.diff(commit.parents[0]) if diff.a_path.endswith(filters)]\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    def get_source_at_commit(self, file_name):\n",
    "        try:\n",
    "            repo = git.Repo(self.repo_path)\n",
    "            commit = repo.commit(self.sha)\n",
    "            return commit.tree[file_name].data_stream.read().decode('utf-8')\n",
    "        except:\n",
    "            return ''\n",
    "\n",
    "    def get_source_at_parent(self, file_name):\n",
    "        try:\n",
    "            repo = git.Repo(self.repo_path)\n",
    "            commit = repo.commit(self.sha)\n",
    "            return commit.parents[0].tree[file_name].data_stream.read().decode('utf-8')\n",
    "        except:\n",
    "            return ''\n",
    "\n",
    "    def source_to_ast(self, source, file_name):\n",
    "        try:\n",
    "            parser = tree_sitter.Parser()\n",
    "            if file_name.endswith('.c'):\n",
    "                parser.set_language(C_LANGUAGE)\n",
    "            elif file_name.endswith('.cpp'):\n",
    "                parser.set_language(CPP_LANGUAGE)\n",
    "            elif file_name.endswith('.java'):\n",
    "                parser.set_language(JAVA_LANGUAGE)\n",
    "            elif file_name.endswith('.js'):\n",
    "                parser.set_language(JS_LANGUAGE)\n",
    "            elif file_name.endswith('.py'):\n",
    "                parser.set_language(PY_LANGUAGE)\n",
    "            else:\n",
    "                print(\"UNKNOWN LANGUAGE\")\n",
    "                return None\n",
    "            return parser.parse(bytes(source, 'utf8'))\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    def get_root_paths(self, tree):\n",
    "        try:\n",
    "            paths = set()\n",
    "\n",
    "            # Recursive function to explore the tree\n",
    "            def explore(node, path, terminalA=None):\n",
    "                # Add current node to path\n",
    "                if terminalA is None:\n",
    "                    terminalA = node\n",
    "                path.append(node.type)\n",
    "\n",
    "                # If the node has no children, it's a leaf node and the path is complete\n",
    "                if not node.child_count:\n",
    "                    if USE_FULL_TUPLES:\n",
    "                        paths.add((terminalA.text, tuple(path), node.text))\n",
    "                    else:\n",
    "                        paths.add(tuple(path))\n",
    "                else:\n",
    "                    # Explore each child node recursively\n",
    "                    for child in node.children:\n",
    "                        explore(child, path, terminalA)\n",
    "\n",
    "                # Remove current node from path before returning\n",
    "                path.pop()\n",
    "\n",
    "            # Start exploring from the root node\n",
    "            root_node = tree.root_node\n",
    "            explore(root_node, [])\n",
    "            \n",
    "            return paths\n",
    "        except:\n",
    "            return set([])\n",
    "\n",
    "    def get_paths(self, ast):\n",
    "        graph = nx.Graph()\n",
    "        node_id = 0\n",
    "        leaf_nodes = []\n",
    "\n",
    "        def add_node(node):\n",
    "            nonlocal node_id\n",
    "            node_name = node.type\n",
    "            node_id += 1\n",
    "            graph.add_node(node_id, name=node_name, node=node, text=node.text)\n",
    "            if not node.children:\n",
    "                leaf_nodes.append(node_id)\n",
    "            return node_id\n",
    "\n",
    "        def add_edge(parent_id, child_id):\n",
    "            graph.add_edge(parent_id, child_id)\n",
    "\n",
    "        def traverse(node, parent_id=None):\n",
    "            current_id = add_node(node)\n",
    "            if parent_id is not None:\n",
    "                add_edge(parent_id, current_id)\n",
    "            for child in node.children:\n",
    "                traverse(child, current_id)\n",
    "\n",
    "        traverse(ast.root_node)\n",
    "\n",
    "        if BAG_SIZE != None:\n",
    "            leaf_nodes_sample = random.sample(leaf_nodes, min(BAG_SIZE, len(leaf_nodes)))\n",
    "        else:\n",
    "            leaf_nodes_sample = leaf_nodes\n",
    "        leaf_node_pairs = [(leaf_nodes_sample[i], leaf_nodes_sample[j]) for i in range(len(leaf_nodes_sample)) for j in range(i+1, len(leaf_nodes_sample))]\n",
    "        \n",
    "        if BAG_SIZE != None:\n",
    "            leaf_node_pairs = random.sample(leaf_node_pairs, min(BAG_SIZE,len(leaf_node_pairs)))\n",
    "        \n",
    "        all_paths = []\n",
    "        for pair in leaf_node_pairs:\n",
    "            path = list(nx.all_simple_paths(graph, source=pair[0], target=pair[1]))[0]\n",
    "            node_types = [graph.nodes[nodeID]['name'] for nodeID in path]\n",
    "            if(USE_FULL_TUPLES):\n",
    "                all_paths.append((graph.nodes[pair[0]]['text'], tuple(node_types), graph.nodes[pair[1]]['text']))\n",
    "            else:\n",
    "                all_paths.append(tuple(node_types))\n",
    "                \n",
    "        return set(all_paths)\n",
    "\n",
    "\n",
    "    def ast_to_bag_of_contexts(self, ast_trees):\n",
    "        paths = set()\n",
    "        for tree in ast_trees:\n",
    "            if(ONLY_ROOT_PATHS):\n",
    "                paths |= self.get_root_paths(tree)\n",
    "            else:\n",
    "                paths |= self.get_paths(tree)\n",
    "        return paths\n",
    "\n",
    "    def map_bag_of_contexts_to_id(self, bag_of_contexts):\n",
    "        mapped_paths = []\n",
    "        for path in bag_of_contexts:\n",
    "            mapped_path = []\n",
    "            for node in path:\n",
    "                index = ALL_NODE_TYPES.index(node)\n",
    "                mapped_path.append(index + 1)\n",
    "            mapped_paths.append(mapped_path)\n",
    "        return mapped_paths\n",
    "\n",
    "    def one_hot_encode(self, bag_of_contexts):\n",
    "        one_hot_paths = []\n",
    "\n",
    "        # Iterate over each row in the array\n",
    "        for row in bag_of_contexts:\n",
    "            # Create an empty list to hold the one-hot encodings for this row\n",
    "            row_one_hot = []\n",
    "\n",
    "            # Iterate over each element in the row\n",
    "            for num in row:\n",
    "                # Create an empty list to hold the one-hot encoding for this number\n",
    "                num_one_hot = [0] * (MAX_NODE_LOOKUP_NUM + 1)\n",
    "\n",
    "                # Set the corresponding element to 1\n",
    "                num_one_hot[int(num)] = 1\n",
    "\n",
    "                # Add the one-hot encoding for this number to the row's list\n",
    "                row_one_hot.append(num_one_hot)\n",
    "\n",
    "            # Add the row's list of one-hot encodings to the main list\n",
    "            one_hot_paths.append(row_one_hot)\n",
    "\n",
    "        return one_hot_paths\n",
    "\n",
    "    def pad_each_context(self, bag_of_contexts):\n",
    "        padded_one_hot_paths = []\n",
    "        for path in bag_of_contexts:\n",
    "            if IGNORE_DEEP_PATHS and len(path) > CONTEXT_SIZE:\n",
    "                continue\n",
    "            if ONE_HOT:\n",
    "                padded_path = [[0] * (MAX_NODE_LOOKUP_NUM + 1)] * max(CONTEXT_SIZE - len(path), 0) + path[-CONTEXT_SIZE:]\n",
    "            else:\n",
    "                padded_path = [0] * max(CONTEXT_SIZE - len(path), 0) + path[-CONTEXT_SIZE:]\n",
    "            padded_one_hot_paths.append(padded_path)\n",
    "        return padded_one_hot_paths\n",
    "\n",
    "    def to_raw_consumable_data(self):\n",
    "        files_changed = self.get_files_changed()\n",
    "        sources_at_commit = [self.get_source_at_commit(filename) for filename in files_changed]\n",
    "        sources_at_parent = [self.get_source_at_parent(filename) for filename in files_changed]        \n",
    "\n",
    "        asts_commit = [self.source_to_ast(source, files_changed[i]) for i, source in enumerate(sources_at_commit)]\n",
    "        asts_parent = [self.source_to_ast(source, files_changed[i]) for i, source in enumerate(sources_at_parent)]\n",
    "        \n",
    "        contexts_commit = self.ast_to_bag_of_contexts(asts_commit)\n",
    "        contexts_parent = self.ast_to_bag_of_contexts(asts_parent)\n",
    "\n",
    "        contexts = list(contexts_commit.symmetric_difference(contexts_parent))\n",
    "\n",
    "        \n",
    "        contexts = self.map_bag_of_contexts_to_id(contexts)\n",
    "\n",
    "        if(ONE_HOT):\n",
    "            contexts = self.one_hot_encode(contexts)\n",
    "\n",
    "        contexts = self.pad_each_context(contexts)\n",
    "\n",
    "        return contexts\n",
    "    \n",
    "    def raw_to_padded(self, consumable):\n",
    "        if(len(consumable) == BAG_SIZE):\n",
    "            return consumable\n",
    "        \n",
    "        if(len(consumable) > BAG_SIZE):\n",
    "            return random.sample(consumable, BAG_SIZE)\n",
    "        \n",
    "        if ONE_HOT:\n",
    "            blank_path = [[0] * (MAX_NODE_LOOKUP_NUM + 1)] * CONTEXT_SIZE\n",
    "        else:\n",
    "            blank_path = ([0] * CONTEXT_SIZE)\n",
    "            \n",
    "        return ([blank_path] * (consumable - len(consumable)) + consumable)\n",
    "\n",
    "    \n",
    "    def to_padded_consumable_data(self):\n",
    "        consumable = self.to_raw_consumable_data()\n",
    "        padded = self.raw_to_padded(consumable)\n",
    "        if(ONE_HOT):\n",
    "            padded = np.array(padded)\n",
    "            padded = padded.reshape(padded.shape[0], padded.shape[1] * padded.shape[2])\n",
    "            padded = padded.tolist()\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fdf9c3-07e2-4912-8570-0614f7b9fdbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a132ce3f231a454fabc3ffafad7f8ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Creates a lookup dictionary where any commit SHA can be looked up to grab the Commit object with all the data, + bag of paths\n",
    "COMMIT_DATA_LOOKUP = defaultdict(list)\n",
    "\n",
    "for index, row in tqdm(RAW_DATA.iterrows(), total=RAW_DATA.shape[0]):\n",
    "    fix_hash = row[\"fix_hash\"]\n",
    "    bug_hash = row[\"bug_hash\"]\n",
    "    repo = row[\"repo\"]\n",
    "    repo_folder = repo.replace(\"/\", \"-\")\n",
    "    repo_path = f\"clones/{repo_folder}\"\n",
    "\n",
    "    # Loop over the fix_hash and bug_hash\n",
    "    for sha in [fix_hash, bug_hash]:\n",
    "        if sha not in COMMIT_DATA_LOOKUP:\n",
    "            # Create a new Commit object for the sha and add it to the commit_map\n",
    "            commit = Commit(sha, repo_path)\n",
    "            commit._populate_commit_info()\n",
    "            commit._generate_bags_of_contexts()\n",
    "            COMMIT_DATA_LOOKUP[sha] = commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6386a9f7-faae-4d6e-b441-6656c9c648e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Preprocess each commit metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5add242-33c0-4b00-afff-33485dfe6e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Create X_Train and X_Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f726de46-3e71-4172-8e3d-46fb4f4352e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Declare params for ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5f56908-0f3d-476e-aadb-2e4bdf6485ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f390403-cb08-48f1-9dec-693a128a0005",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Train SimSiam Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0b9318f-b5f4-40cb-bf1c-118f77d931ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Train binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b491ea8e-03e2-4408-9163-7078c3a9b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Evaluate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
